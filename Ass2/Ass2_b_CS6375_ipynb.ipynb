{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "DrWJ4fAX3Yo1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import unicodedata\n",
        "import math\n",
        "import copy\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from zipfile import ZipFile\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "WLvFSsV1OqOi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "RV1m05wK3mvj"
      },
      "outputs": [],
      "source": [
        "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True)\n",
        "X = X / 255"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "xHjO-Ju3DqGK",
        "outputId": "a3afa5ad-36ad-41ff-92cc-2d6e6f8df0fb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  pixel9  \\\n",
              "0         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
              "1         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
              "2         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
              "3         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
              "4         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
              "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
              "69995     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
              "69996     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
              "69997     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
              "69998     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
              "69999     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
              "\n",
              "       pixel10  ...  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
              "0          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
              "1          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
              "2          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
              "3          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
              "4          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
              "...        ...  ...       ...       ...       ...       ...       ...   \n",
              "69995      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
              "69996      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
              "69997      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
              "69998      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
              "69999      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
              "\n",
              "       pixel780  pixel781  pixel782  pixel783  pixel784  \n",
              "0           0.0       0.0       0.0       0.0       0.0  \n",
              "1           0.0       0.0       0.0       0.0       0.0  \n",
              "2           0.0       0.0       0.0       0.0       0.0  \n",
              "3           0.0       0.0       0.0       0.0       0.0  \n",
              "4           0.0       0.0       0.0       0.0       0.0  \n",
              "...         ...       ...       ...       ...       ...  \n",
              "69995       0.0       0.0       0.0       0.0       0.0  \n",
              "69996       0.0       0.0       0.0       0.0       0.0  \n",
              "69997       0.0       0.0       0.0       0.0       0.0  \n",
              "69998       0.0       0.0       0.0       0.0       0.0  \n",
              "69999       0.0       0.0       0.0       0.0       0.0  \n",
              "\n",
              "[70000 rows x 784 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dcb9c940-67e2-46d5-84e6-6fd41424e694\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pixel1</th>\n",
              "      <th>pixel2</th>\n",
              "      <th>pixel3</th>\n",
              "      <th>pixel4</th>\n",
              "      <th>pixel5</th>\n",
              "      <th>pixel6</th>\n",
              "      <th>pixel7</th>\n",
              "      <th>pixel8</th>\n",
              "      <th>pixel9</th>\n",
              "      <th>pixel10</th>\n",
              "      <th>...</th>\n",
              "      <th>pixel775</th>\n",
              "      <th>pixel776</th>\n",
              "      <th>pixel777</th>\n",
              "      <th>pixel778</th>\n",
              "      <th>pixel779</th>\n",
              "      <th>pixel780</th>\n",
              "      <th>pixel781</th>\n",
              "      <th>pixel782</th>\n",
              "      <th>pixel783</th>\n",
              "      <th>pixel784</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69995</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69996</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69997</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69998</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69999</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>70000 rows Ã— 784 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dcb9c940-67e2-46d5-84e6-6fd41424e694')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dcb9c940-67e2-46d5-84e6-6fd41424e694 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dcb9c940-67e2-46d5-84e6-6fd41424e694');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2mETRYUe3otF"
      },
      "outputs": [],
      "source": [
        "X_train, X_test = X[:60000], X[60000:]\n",
        "y_train, y_test = y[:60000], y[60000:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "rslfVLDR4E57",
        "outputId": "1bbdd835-6522-4a88-d09d-5deedfa7fdea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f806fb960d0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.imshow(X_train.iloc[10].values.reshape(28,28))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jbPCiXbk4S4b"
      },
      "outputs": [],
      "source": [
        "# X_train.iloc[10].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TMjT5fdz58PW"
      },
      "outputs": [],
      "source": [
        "# X_train = X_train/255.0\n",
        "# X_train.iloc[10].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0Yw4V4TQ6pTL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U011Kxob7kNi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>SVM</h1>"
      ],
      "metadata": {
        "id": "yJFPtpDlFvrO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vEj75OMn7kD5"
      },
      "outputs": [],
      "source": [
        "C = [0.1, 1, 10]\n",
        "kernal = [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\n",
        "degree_for_poly = [3, 4]\n",
        "gamma = [\"scale\", \"auto\"]\n",
        "SVM_models = dict()\n",
        "for k in kernal:\n",
        "    for c in C:\n",
        "        model_linear = SVC(kernel=k, C=c, max_iter=256)\n",
        "        model_linear.fit(X_train, y_train)\n",
        "        SVM_models[k+str(c)] = model_linear"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# predict\n",
        "SVM_predicts = dict()\n",
        "for k in kernal:\n",
        "    for c in C:\n",
        "        SVM_predicts[k+str(c)] = SVM_models[k+str(c)].predict(X_test)"
      ],
      "metadata": {
        "id": "mh16n-nnoMS5"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "for k in kernal:\n",
        "    for c in C:\n",
        "        # accuracy\n",
        "        i += 1\n",
        "        print(f\"{i}) Model: SVM, Parameters: kernal: {k}, Reg parameter C: {str(c)}, accuracy: {metrics.accuracy_score(y_true=y_test, y_pred=SVM_predicts[k+str(c)])}, Absolute Error: {metrics.mean_absolute_error(y_true=y_test, y_pred=SVM_predicts[k+str(c)])}, Mean Squared Error: {metrics.mean_squared_error(y_true=y_test, y_pred=SVM_predicts[k+str(c)])} \\n\")\n",
        "\n",
        "        # cm\n",
        "        # print(metrics.confusion_matrix(y_true=y_test, y_pred=SVM_predicts[k+str(c)]))\n",
        "        "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCCXz0GtPjGN",
        "outputId": "d3b8a261-d128-4b66-b913-1c135a14742e"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1) Model: SVM, Parameters: kernal: linear, Reg parameter C: 0.1, accuracy: 0.8246, Absolute Error: 0.5946, Mean Squared Error: 2.6294 \n",
            "\n",
            "2) Model: SVM, Parameters: kernal: linear, Reg parameter C: 1, accuracy: 0.7485, Absolute Error: 0.8713, Mean Squared Error: 3.9459 \n",
            "\n",
            "3) Model: SVM, Parameters: kernal: linear, Reg parameter C: 10, accuracy: 0.7481, Absolute Error: 0.8741, Mean Squared Error: 3.9597 \n",
            "\n",
            "4) Model: SVM, Parameters: kernal: poly, Reg parameter C: 0.1, accuracy: 0.3469, Absolute Error: 3.036, Mean Squared Error: 17.6612 \n",
            "\n",
            "5) Model: SVM, Parameters: kernal: poly, Reg parameter C: 1, accuracy: 0.8455, Absolute Error: 0.5299, Mean Squared Error: 2.2545 \n",
            "\n",
            "6) Model: SVM, Parameters: kernal: poly, Reg parameter C: 10, accuracy: 0.975, Absolute Error: 0.0977, Mean Squared Error: 0.4857 \n",
            "\n",
            "7) Model: SVM, Parameters: kernal: rbf, Reg parameter C: 0.1, accuracy: 0.8723, Absolute Error: 0.465, Mean Squared Error: 2.1346 \n",
            "\n",
            "8) Model: SVM, Parameters: kernal: rbf, Reg parameter C: 1, accuracy: 0.9764, Absolute Error: 0.094, Mean Squared Error: 0.47 \n",
            "\n",
            "9) Model: SVM, Parameters: kernal: rbf, Reg parameter C: 10, accuracy: 0.9822, Absolute Error: 0.07, Mean Squared Error: 0.3512 \n",
            "\n",
            "10) Model: SVM, Parameters: kernal: sigmoid, Reg parameter C: 0.1, accuracy: 0.3979, Absolute Error: 2.455, Mean Squared Error: 12.6086 \n",
            "\n",
            "11) Model: SVM, Parameters: kernal: sigmoid, Reg parameter C: 1, accuracy: 0.547, Absolute Error: 1.8111, Mean Squared Error: 8.8109 \n",
            "\n",
            "12) Model: SVM, Parameters: kernal: sigmoid, Reg parameter C: 10, accuracy: 0.6069, Absolute Error: 1.492, Mean Squared Error: 7.0926 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for k in kernal:\n",
        "    for c in C:\n",
        "        # accuracy\n",
        "        print(f\"Model: SVM, Parameters: kernal: {k}, Reg parameter C: {str(c)}, accuracy: {metrics.accuracy_score(y_true=y_test, y_pred=SVM_predicts[k+str(c)])} \\n\")\n",
        "\n",
        "        # cm\n",
        "        print(metrics.confusion_matrix(y_true=y_test, y_pred=SVM_predicts[k+str(c)]))\n",
        "        "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGIlu0F7rJAX",
        "outputId": "309dc478-38b4-462c-bd88-896d0b9a41fe"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: SVM, Parameters: kernal: linear, Reg parameter C: 0.1, accuracy: 0.8246 \n",
            "\n",
            "[[ 956    0    4    1    1   13    2    1    1    1]\n",
            " [   0 1108    1    3    0    1    2    1   19    0]\n",
            " [  15   38  857   52    9    7   18    6   28    2]\n",
            " [   5    8   61  744    1  138    0   14   32    7]\n",
            " [   1    2    7    2  865    1    5    2    2   95]\n",
            " [  23   17   13   98   14  601   16    1  106    3]\n",
            " [   7    4    9    0    6   22  906    1    3    0]\n",
            " [   0    8   23    9    7    0    0  834    1  146]\n",
            " [  17   15   81  107   12   39   15    9  660   19]\n",
            " [   9    7    7   19  117    4    0  125    6  715]]\n",
            "Model: SVM, Parameters: kernal: linear, Reg parameter C: 1, accuracy: 0.7485 \n",
            "\n",
            "[[ 924    0   18    0    1   19    9    2    7    0]\n",
            " [   0 1058    0    4    0    1    2    0   69    1]\n",
            " [  24   90  775   51   14    8   20   14   35    1]\n",
            " [   9   29   73  616    2  216    1   17   30   17]\n",
            " [   3    3   14    2  811    3   10   14   10  112]\n",
            " [  26    5   15  121   10  607   20    2   67   19]\n",
            " [  24    4   43    0   38   47  798    0    4    0]\n",
            " [   2    7   10   83   42    0    0  743    8  133]\n",
            " [  31   43   45  144   13   45   15    7  624    7]\n",
            " [  10   10    9   16  235    7    0  154   39  529]]\n",
            "Model: SVM, Parameters: kernal: linear, Reg parameter C: 10, accuracy: 0.7481 \n",
            "\n",
            "[[ 923    0   18    0    2   19    9    2    7    0]\n",
            " [   0 1055    0    4    0    1    2    3   70    0]\n",
            " [  24   86  778   51   15    8   20   14   35    1]\n",
            " [   9   30   73  617    2  215    1   18   30   15]\n",
            " [   3    2   14    2  813    2   10   13   10  113]\n",
            " [  26    6   15  121   10  606   20    2   67   19]\n",
            " [  24    4   43    0   39   47  797    0    4    0]\n",
            " [   2   10   10   82   44    0    0  739    8  133]\n",
            " [  31   43   45  143   14   45   15    7  624    7]\n",
            " [  10   10    9   16  233    7    0  156   39  529]]\n",
            "Model: SVM, Parameters: kernal: poly, Reg parameter C: 0.1, accuracy: 0.3469 \n",
            "\n",
            "[[475   0   0   0   0   1   7   0 198 299]\n",
            " [  0 675   0   0   0   0   0   0  50 410]\n",
            " [  2   5  70   0   1   2  20   6 423 503]\n",
            " [  0   0   1 135   0  12   1   2 319 540]\n",
            " [  1   0   0   0  74   0   2   0   2 903]\n",
            " [  1   0   0   5   0 131  10   0  46 699]\n",
            " [  1   1   1   0   0   0 460   0 182 313]\n",
            " [  1   0   0   0   0   0   0 161   6 860]\n",
            " [  1   3   0   4   1   0   5   2 301 657]\n",
            " [  2   0   0   1   7   4   1   3   4 987]]\n",
            "Model: SVM, Parameters: kernal: poly, Reg parameter C: 1, accuracy: 0.8455 \n",
            "\n",
            "[[ 970    0    0    0    0    1    3    0    3    3]\n",
            " [   0 1121    1    2    0    0    3    1    5    2]\n",
            " [   7    1  990    2    0    0    5   11   11    5]\n",
            " [   0    0   18  856    0   60    1    7   54   14]\n",
            " [   1    4    3    0  535    0    5    2    1  431]\n",
            " [   2    1    2   17    0  820    9    3   18   20]\n",
            " [   5    2    1    0    0    2  943    1    3    1]\n",
            " [   1   14   38   16    1    0    1  308    1  648]\n",
            " [   4    0    1    9    2    1    0    2  938   17]\n",
            " [   5    6    3    6   11    2    0    2    0  974]]\n",
            "Model: SVM, Parameters: kernal: poly, Reg parameter C: 10, accuracy: 0.975 \n",
            "\n",
            "[[ 972    0    1    1    0    4    1    0    1    0]\n",
            " [   0 1128    2    1    0    0    4    0    0    0]\n",
            " [   7    2 1008    1    2    0    3    6    3    0]\n",
            " [   1    1    4  980    0   10    0    4    5    5]\n",
            " [   2    0    1    0  966    0    3    1    0    9]\n",
            " [   3    0    2   19    1  859    3    1    2    2]\n",
            " [   4    5    2    0    2    5  938    0    2    0]\n",
            " [   0   10   12    3    2    0    0  991    0   10]\n",
            " [   6    0    2    8    3    6    1    4  941    3]\n",
            " [   5    4    1    7   15    3    0    3    4  967]]\n",
            "Model: SVM, Parameters: kernal: rbf, Reg parameter C: 0.1, accuracy: 0.8723 \n",
            "\n",
            "[[ 961    0    1    0    0    1   10    1    5    1]\n",
            " [   0 1113    0    3    0    0    3    0   13    3]\n",
            " [   6    6  831   24    7    3   18   37   90   10]\n",
            " [   0    0    1  850    1   42    3    8   55   50]\n",
            " [   1    0    0    0  707    0   10    1    3  260]\n",
            " [   2    1    0    8    3  777   18    4   44   35]\n",
            " [   3    3    1    1    1    3  935    1   10    0]\n",
            " [   0    4    1    4    9    0    0  708    6  296]\n",
            " [   2    0    0    2    4    4    1    3  886   72]\n",
            " [   6    2    1    2   13    3    1   25    1  955]]\n",
            "Model: SVM, Parameters: kernal: rbf, Reg parameter C: 1, accuracy: 0.9764 \n",
            "\n",
            "[[ 972    0    1    0    0    2    2    1    2    0]\n",
            " [   0 1126    3    1    0    1    1    1    2    0]\n",
            " [   6    2 1001    3    1    0    2    9    7    1]\n",
            " [   0    0    1  986    1   11    0    2    6    3]\n",
            " [   0    0    3    0  952    0    4    1    2   20]\n",
            " [   2    0    0    6    0  871    4    1    7    1]\n",
            " [   6    2    0    0    2    3  943    0    2    0]\n",
            " [   0    7   11    1    3    0    0  990    0   16]\n",
            " [   3    0    3    5    4    3    1    4  948    3]\n",
            " [   3    4    1    7   10    2    1    5    1  975]]\n",
            "Model: SVM, Parameters: kernal: rbf, Reg parameter C: 10, accuracy: 0.9822 \n",
            "\n",
            "[[ 973    0    1    1    0    2    1    1    1    0]\n",
            " [   0 1131    1    0    0    1    0    1    1    0]\n",
            " [   5    1 1014    1    1    0    1    6    3    0]\n",
            " [   0    0    3  992    0    7    0    2    5    1]\n",
            " [   1    0    2    0  966    0    3    1    0    9]\n",
            " [   2    0    0    9    1  872    3    0    3    2]\n",
            " [   4    2    0    0    2    3  946    0    1    0]\n",
            " [   0    4    8    3    1    0    0 1005    0    7]\n",
            " [   3    0    3    6    3    1    1    2  950    5]\n",
            " [   3    4    0    8   10    2    0    7    2  973]]\n",
            "Model: SVM, Parameters: kernal: sigmoid, Reg parameter C: 0.1, accuracy: 0.3979 \n",
            "\n",
            "[[362   0  13   0   0  97 201   0 246  61]\n",
            " [  0 814  27   3   0   6   7   1 269   8]\n",
            " [  3  18 159   3   8   7 313  12 444  65]\n",
            " [  3   2  14  31   0 224  48   8 471 209]\n",
            " [  0   1   0   0  52   6  37   2   7 877]\n",
            " [ 12   7   6   5  13 315  83   1 222 228]\n",
            " [  3   2  21   0   7  11 884   0  24   6]\n",
            " [  0   4   8   0   1   3   2 103  39 868]\n",
            " [  7   3  16   3   6  40 101   0 283 515]\n",
            " [  2   2   0   0   5   9   2   6   7 976]]\n",
            "Model: SVM, Parameters: kernal: sigmoid, Reg parameter C: 1, accuracy: 0.547 \n",
            "\n",
            "[[ 716    0    5    0    0   83   72    0   98    6]\n",
            " [   0 1028    6    1    0    3    4    0   91    2]\n",
            " [  33   88  304    4   12    7  155    9  390   30]\n",
            " [  35   21   18  118    1  219   15   16  528   39]\n",
            " [   1    3    5    0  250    6   27    4   21  665]\n",
            " [  42   28    3   20   15  312   26    7  384   55]\n",
            " [  18    6   52    1   20   17  787    0   55    2]\n",
            " [   6   15   13    0    8    1    3  370   42  570]\n",
            " [  27   40   24    3   17   31   16    8  702  106]\n",
            " [  12    6    7    0   41   10    0   24   26  883]]\n",
            "Model: SVM, Parameters: kernal: sigmoid, Reg parameter C: 10, accuracy: 0.6069 \n",
            "\n",
            "[[754   0   5   0   1  87  49   1  81   2]\n",
            " [  0 999  31   1   0   3   5   0  96   0]\n",
            " [ 45  79 393   7  38  13 162  11 264  20]\n",
            " [ 47  15  15 146   2 244  15  17 483  26]\n",
            " [  1   4   5   0 708   7  19   6  22 210]\n",
            " [ 52  23   5  19  30 357  22   9 356  19]\n",
            " [ 25   5  47   2  44  15 758   0  60   2]\n",
            " [  8  18  14   4  14   2   5 455  39 469]\n",
            " [ 42  36  16   8  20  45  11  10 724  62]\n",
            " [ 15   6   1   0 109  16   1  41  45 775]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "W17Xy3co8qs1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>MLPClassifier AKA Neural Networks</h1>"
      ],
      "metadata": {
        "id": "hZ2U0hcJ0ncz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_layers = [10, 100]\n",
        "activations = [\"logistic\", \"tanh\", \"relu\"]\n",
        "optimizers = [\"sgd\", \"adam\"]\n",
        "reg_parameters = [0.001, 0.1]\n",
        "lr_parameters = [0.001, 0.01, 0.1]\n",
        "NN_models = dict()\n",
        "for h in hidden_layers:\n",
        "    for a in activations:\n",
        "        for o in optimizers:\n",
        "            for reg in reg_parameters:\n",
        "                for lr in lr_parameters:\n",
        "                    model = MLPClassifier(random_state=1, hidden_layer_sizes=(h, ), activation=a, solver=o, alpha=reg, learning_rate_init=lr, max_iter=64)\n",
        "                    model.fit(X_train, y_train)\n",
        "                    NN_models[a+str(h)+o+str(reg)+str(lr)] = model \n",
        "                    print(a+str(h)+o+str(reg)+str(lr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TU87OAmP0uPs",
        "outputId": "ab2fad5b-c08f-481b-d346-ea244622ba6b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logistic10sgd0.0010.001\n",
            "logistic10sgd0.0010.01\n",
            "logistic10sgd0.0010.1\n",
            "logistic10sgd0.10.001\n",
            "logistic10sgd0.10.01\n",
            "logistic10sgd0.10.1\n",
            "logistic10adam0.0010.001\n",
            "logistic10adam0.0010.01\n",
            "logistic10adam0.0010.1\n",
            "logistic10adam0.10.001\n",
            "logistic10adam0.10.01\n",
            "logistic10adam0.10.1\n",
            "tanh10sgd0.0010.001\n",
            "tanh10sgd0.0010.01\n",
            "tanh10sgd0.0010.1\n",
            "tanh10sgd0.10.001\n",
            "tanh10sgd0.10.01\n",
            "tanh10sgd0.10.1\n",
            "tanh10adam0.0010.001\n",
            "tanh10adam0.0010.01\n",
            "tanh10adam0.0010.1\n",
            "tanh10adam0.10.001\n",
            "tanh10adam0.10.01\n",
            "tanh10adam0.10.1\n",
            "relu10sgd0.0010.001\n",
            "relu10sgd0.0010.01\n",
            "relu10sgd0.0010.1\n",
            "relu10sgd0.10.001\n",
            "relu10sgd0.10.01\n",
            "relu10sgd0.10.1\n",
            "relu10adam0.0010.001\n",
            "relu10adam0.0010.01\n",
            "relu10adam0.0010.1\n",
            "relu10adam0.10.001\n",
            "relu10adam0.10.01\n",
            "relu10adam0.10.1\n",
            "logistic100sgd0.0010.001\n",
            "logistic100sgd0.0010.01\n",
            "logistic100sgd0.0010.1\n",
            "logistic100sgd0.10.001\n",
            "logistic100sgd0.10.01\n",
            "logistic100sgd0.10.1\n",
            "logistic100adam0.0010.001\n",
            "logistic100adam0.0010.01\n",
            "logistic100adam0.0010.1\n",
            "logistic100adam0.10.001\n",
            "logistic100adam0.10.01\n",
            "logistic100adam0.10.1\n",
            "tanh100sgd0.0010.001\n",
            "tanh100sgd0.0010.01\n",
            "tanh100sgd0.0010.1\n",
            "tanh100sgd0.10.001\n",
            "tanh100sgd0.10.01\n",
            "tanh100sgd0.10.1\n",
            "tanh100adam0.0010.001\n",
            "tanh100adam0.0010.01\n",
            "tanh100adam0.0010.1\n",
            "tanh100adam0.10.001\n",
            "tanh100adam0.10.01\n",
            "tanh100adam0.10.1\n",
            "relu100sgd0.0010.001\n",
            "relu100sgd0.0010.01\n",
            "relu100sgd0.0010.1\n",
            "relu100sgd0.10.001\n",
            "relu100sgd0.10.01\n",
            "relu100sgd0.10.1\n",
            "relu100adam0.0010.001\n",
            "relu100adam0.0010.01\n",
            "relu100adam0.0010.1\n",
            "relu100adam0.10.001\n",
            "relu100adam0.10.01\n",
            "relu100adam0.10.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# predict\n",
        "NN_predicts = dict()\n",
        "for h in hidden_layers:\n",
        "    for a in activations:\n",
        "        for o in optimizers:\n",
        "            for reg in reg_parameters:\n",
        "                for lr in lr_parameters:\n",
        "                        NN_predicts[a+str(h)+o+str(reg)+str(lr)] = NN_models[a+str(h)+o+str(reg)+str(lr)].predict(X_test)\n",
        "        print(str(h), a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6KL-nU833ie",
        "outputId": "81b42a46-c546-4bfb-82db-44f9d63065ee"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 logistic\n",
            "10 tanh\n",
            "10 relu\n",
            "100 logistic\n",
            "100 tanh\n",
            "100 relu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "for h in hidden_layers:\n",
        "    for a in activations:\n",
        "        for o in optimizers:\n",
        "            for reg in reg_parameters:\n",
        "                for lr in lr_parameters:\n",
        "                    # accuracy\n",
        "                    i += 1\n",
        "                    print(f\"{i}) Model: NN, Parameters: No. hidden layers: {h}, Activation: {a},  Optimizer: {o}, Reg Para: {reg}, Learning Rate: {lr}, accuracy: {metrics.accuracy_score(y_true=y_test, y_pred=NN_predicts[a+str(h)+o+str(reg)+str(lr)])}, Absolute Error: {metrics.mean_absolute_error(y_true=y_test, y_pred=NN_predicts[a+str(h)+o+str(reg)+str(lr)])}, Mean Squared Error: {metrics.mean_squared_error(y_true=y_test, y_pred=NN_predicts[a+str(h)+o+str(reg)+str(lr)])} \\n\")\n",
        "\n",
        "                    # cm\n",
        "                    # print(metrics.confusion_matrix(y_true=y_test, y_pred=NN_predicts[a+str(h)+o+str(reg)+str(lr)]))\n",
        "                    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcQYpewiI_mO",
        "outputId": "1136fdfb-be1e-40fe-d14e-a1b5eba03793"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1) Model: NN, Parameters: No. hidden layers: 10, Activation: logistic,  Optimizer: sgd, Reg Para: 0.001, Learning Rate: 0.001, accuracy: 0.8937, Absolute Error: 0.3954, Mean Squared Error: 1.9094 \n",
            "\n",
            "2) Model: NN, Parameters: No. hidden layers: 10, Activation: logistic,  Optimizer: sgd, Reg Para: 0.001, Learning Rate: 0.01, accuracy: 0.9316, Absolute Error: 0.2561, Mean Squared Error: 1.2327 \n",
            "\n",
            "3) Model: NN, Parameters: No. hidden layers: 10, Activation: logistic,  Optimizer: sgd, Reg Para: 0.001, Learning Rate: 0.1, accuracy: 0.9374, Absolute Error: 0.2281, Mean Squared Error: 1.0825 \n",
            "\n",
            "4) Model: NN, Parameters: No. hidden layers: 10, Activation: logistic,  Optimizer: sgd, Reg Para: 0.1, Learning Rate: 0.001, accuracy: 0.8934, Absolute Error: 0.3962, Mean Squared Error: 1.9108 \n",
            "\n",
            "5) Model: NN, Parameters: No. hidden layers: 10, Activation: logistic,  Optimizer: sgd, Reg Para: 0.1, Learning Rate: 0.01, accuracy: 0.9261, Absolute Error: 0.2779, Mean Squared Error: 1.3473 \n",
            "\n",
            "6) Model: NN, Parameters: No. hidden layers: 10, Activation: logistic,  Optimizer: sgd, Reg Para: 0.1, Learning Rate: 0.1, accuracy: 0.9302, Absolute Error: 0.2649, Mean Squared Error: 1.2871 \n",
            "\n",
            "7) Model: NN, Parameters: No. hidden layers: 10, Activation: logistic,  Optimizer: adam, Reg Para: 0.001, Learning Rate: 0.001, accuracy: 0.9352, Absolute Error: 0.2397, Mean Squared Error: 1.1561 \n",
            "\n",
            "8) Model: NN, Parameters: No. hidden layers: 10, Activation: logistic,  Optimizer: adam, Reg Para: 0.001, Learning Rate: 0.01, accuracy: 0.9325, Absolute Error: 0.2399, Mean Squared Error: 1.1147 \n",
            "\n",
            "9) Model: NN, Parameters: No. hidden layers: 10, Activation: logistic,  Optimizer: adam, Reg Para: 0.001, Learning Rate: 0.1, accuracy: 0.8983, Absolute Error: 0.3374, Mean Squared Error: 1.501 \n",
            "\n",
            "10) Model: NN, Parameters: No. hidden layers: 10, Activation: logistic,  Optimizer: adam, Reg Para: 0.1, Learning Rate: 0.001, accuracy: 0.9313, Absolute Error: 0.2535, Mean Squared Error: 1.2223 \n",
            "\n",
            "11) Model: NN, Parameters: No. hidden layers: 10, Activation: logistic,  Optimizer: adam, Reg Para: 0.1, Learning Rate: 0.01, accuracy: 0.9289, Absolute Error: 0.2583, Mean Squared Error: 1.2083 \n",
            "\n",
            "12) Model: NN, Parameters: No. hidden layers: 10, Activation: logistic,  Optimizer: adam, Reg Para: 0.1, Learning Rate: 0.1, accuracy: 0.8967, Absolute Error: 0.369, Mean Squared Error: 1.7292 \n",
            "\n",
            "13) Model: NN, Parameters: No. hidden layers: 10, Activation: tanh,  Optimizer: sgd, Reg Para: 0.001, Learning Rate: 0.001, accuracy: 0.9194, Absolute Error: 0.2947, Mean Squared Error: 1.3937 \n",
            "\n",
            "14) Model: NN, Parameters: No. hidden layers: 10, Activation: tanh,  Optimizer: sgd, Reg Para: 0.001, Learning Rate: 0.01, accuracy: 0.9341, Absolute Error: 0.2356, Mean Squared Error: 1.0946 \n",
            "\n",
            "15) Model: NN, Parameters: No. hidden layers: 10, Activation: tanh,  Optimizer: sgd, Reg Para: 0.001, Learning Rate: 0.1, accuracy: 0.9271, Absolute Error: 0.2642, Mean Squared Error: 1.2362 \n",
            "\n",
            "16) Model: NN, Parameters: No. hidden layers: 10, Activation: tanh,  Optimizer: sgd, Reg Para: 0.1, Learning Rate: 0.001, accuracy: 0.9188, Absolute Error: 0.2974, Mean Squared Error: 1.4056 \n",
            "\n",
            "17) Model: NN, Parameters: No. hidden layers: 10, Activation: tanh,  Optimizer: sgd, Reg Para: 0.1, Learning Rate: 0.01, accuracy: 0.9342, Absolute Error: 0.2377, Mean Squared Error: 1.1251 \n",
            "\n",
            "18) Model: NN, Parameters: No. hidden layers: 10, Activation: tanh,  Optimizer: sgd, Reg Para: 0.1, Learning Rate: 0.1, accuracy: 0.9366, Absolute Error: 0.2309, Mean Squared Error: 1.0831 \n",
            "\n",
            "19) Model: NN, Parameters: No. hidden layers: 10, Activation: tanh,  Optimizer: adam, Reg Para: 0.001, Learning Rate: 0.001, accuracy: 0.9338, Absolute Error: 0.2447, Mean Squared Error: 1.1723 \n",
            "\n",
            "20) Model: NN, Parameters: No. hidden layers: 10, Activation: tanh,  Optimizer: adam, Reg Para: 0.001, Learning Rate: 0.01, accuracy: 0.9239, Absolute Error: 0.2741, Mean Squared Error: 1.2809 \n",
            "\n",
            "21) Model: NN, Parameters: No. hidden layers: 10, Activation: tanh,  Optimizer: adam, Reg Para: 0.001, Learning Rate: 0.1, accuracy: 0.8842, Absolute Error: 0.434, Mean Squared Error: 2.1624 \n",
            "\n",
            "22) Model: NN, Parameters: No. hidden layers: 10, Activation: tanh,  Optimizer: adam, Reg Para: 0.1, Learning Rate: 0.001, accuracy: 0.9367, Absolute Error: 0.2377, Mean Squared Error: 1.1479 \n",
            "\n",
            "23) Model: NN, Parameters: No. hidden layers: 10, Activation: tanh,  Optimizer: adam, Reg Para: 0.1, Learning Rate: 0.01, accuracy: 0.9349, Absolute Error: 0.2441, Mean Squared Error: 1.1715 \n",
            "\n",
            "24) Model: NN, Parameters: No. hidden layers: 10, Activation: tanh,  Optimizer: adam, Reg Para: 0.1, Learning Rate: 0.1, accuracy: 0.8609, Absolute Error: 0.5261, Mean Squared Error: 2.5817 \n",
            "\n",
            "25) Model: NN, Parameters: No. hidden layers: 10, Activation: relu,  Optimizer: sgd, Reg Para: 0.001, Learning Rate: 0.001, accuracy: 0.9289, Absolute Error: 0.2625, Mean Squared Error: 1.2509 \n",
            "\n",
            "26) Model: NN, Parameters: No. hidden layers: 10, Activation: relu,  Optimizer: sgd, Reg Para: 0.001, Learning Rate: 0.01, accuracy: 0.9381, Absolute Error: 0.2265, Mean Squared Error: 1.0859 \n",
            "\n",
            "27) Model: NN, Parameters: No. hidden layers: 10, Activation: relu,  Optimizer: sgd, Reg Para: 0.001, Learning Rate: 0.1, accuracy: 0.93, Absolute Error: 0.2774, Mean Squared Error: 1.4018 \n",
            "\n",
            "28) Model: NN, Parameters: No. hidden layers: 10, Activation: relu,  Optimizer: sgd, Reg Para: 0.1, Learning Rate: 0.001, accuracy: 0.928, Absolute Error: 0.2666, Mean Squared Error: 1.275 \n",
            "\n",
            "29) Model: NN, Parameters: No. hidden layers: 10, Activation: relu,  Optimizer: sgd, Reg Para: 0.1, Learning Rate: 0.01, accuracy: 0.9403, Absolute Error: 0.2252, Mean Squared Error: 1.0952 \n",
            "\n",
            "30) Model: NN, Parameters: No. hidden layers: 10, Activation: relu,  Optimizer: sgd, Reg Para: 0.1, Learning Rate: 0.1, accuracy: 0.9187, Absolute Error: 0.2961, Mean Squared Error: 1.4061 \n",
            "\n",
            "31) Model: NN, Parameters: No. hidden layers: 10, Activation: relu,  Optimizer: adam, Reg Para: 0.001, Learning Rate: 0.001, accuracy: 0.9384, Absolute Error: 0.23, Mean Squared Error: 1.118 \n",
            "\n",
            "32) Model: NN, Parameters: No. hidden layers: 10, Activation: relu,  Optimizer: adam, Reg Para: 0.001, Learning Rate: 0.01, accuracy: 0.9214, Absolute Error: 0.2841, Mean Squared Error: 1.3589 \n",
            "\n",
            "33) Model: NN, Parameters: No. hidden layers: 10, Activation: relu,  Optimizer: adam, Reg Para: 0.001, Learning Rate: 0.1, accuracy: 0.6445, Absolute Error: 1.2721, Mean Squared Error: 5.5275 \n",
            "\n",
            "34) Model: NN, Parameters: No. hidden layers: 10, Activation: relu,  Optimizer: adam, Reg Para: 0.1, Learning Rate: 0.001, accuracy: 0.9432, Absolute Error: 0.2069, Mean Squared Error: 1.0117 \n",
            "\n",
            "35) Model: NN, Parameters: No. hidden layers: 10, Activation: relu,  Optimizer: adam, Reg Para: 0.1, Learning Rate: 0.01, accuracy: 0.9165, Absolute Error: 0.3115, Mean Squared Error: 1.4951 \n",
            "\n",
            "36) Model: NN, Parameters: No. hidden layers: 10, Activation: relu,  Optimizer: adam, Reg Para: 0.1, Learning Rate: 0.1, accuracy: 0.3384, Absolute Error: 2.3188, Mean Squared Error: 10.3578 \n",
            "\n",
            "37) Model: NN, Parameters: No. hidden layers: 100, Activation: logistic,  Optimizer: sgd, Reg Para: 0.001, Learning Rate: 0.001, accuracy: 0.913, Absolute Error: 0.3289, Mean Squared Error: 1.6079 \n",
            "\n",
            "38) Model: NN, Parameters: No. hidden layers: 100, Activation: logistic,  Optimizer: sgd, Reg Para: 0.001, Learning Rate: 0.01, accuracy: 0.9631, Absolute Error: 0.147, Mean Squared Error: 0.7368 \n",
            "\n",
            "39) Model: NN, Parameters: No. hidden layers: 100, Activation: logistic,  Optimizer: sgd, Reg Para: 0.001, Learning Rate: 0.1, accuracy: 0.9796, Absolute Error: 0.0809, Mean Squared Error: 0.4065 \n",
            "\n",
            "40) Model: NN, Parameters: No. hidden layers: 100, Activation: logistic,  Optimizer: sgd, Reg Para: 0.1, Learning Rate: 0.001, accuracy: 0.9121, Absolute Error: 0.3319, Mean Squared Error: 1.6239 \n",
            "\n",
            "41) Model: NN, Parameters: No. hidden layers: 100, Activation: logistic,  Optimizer: sgd, Reg Para: 0.1, Learning Rate: 0.01, accuracy: 0.9528, Absolute Error: 0.1821, Mean Squared Error: 0.8941 \n",
            "\n",
            "42) Model: NN, Parameters: No. hidden layers: 100, Activation: logistic,  Optimizer: sgd, Reg Para: 0.1, Learning Rate: 0.1, accuracy: 0.9685, Absolute Error: 0.1256, Mean Squared Error: 0.6276 \n",
            "\n",
            "43) Model: NN, Parameters: No. hidden layers: 100, Activation: logistic,  Optimizer: adam, Reg Para: 0.001, Learning Rate: 0.001, accuracy: 0.9785, Absolute Error: 0.076, Mean Squared Error: 0.3532 \n",
            "\n",
            "44) Model: NN, Parameters: No. hidden layers: 100, Activation: logistic,  Optimizer: adam, Reg Para: 0.001, Learning Rate: 0.01, accuracy: 0.9737, Absolute Error: 0.1009, Mean Squared Error: 0.4959 \n",
            "\n",
            "45) Model: NN, Parameters: No. hidden layers: 100, Activation: logistic,  Optimizer: adam, Reg Para: 0.001, Learning Rate: 0.1, accuracy: 0.9073, Absolute Error: 0.337, Mean Squared Error: 1.5726 \n",
            "\n",
            "46) Model: NN, Parameters: No. hidden layers: 100, Activation: logistic,  Optimizer: adam, Reg Para: 0.1, Learning Rate: 0.001, accuracy: 0.968, Absolute Error: 0.1283, Mean Squared Error: 0.6473 \n",
            "\n",
            "47) Model: NN, Parameters: No. hidden layers: 100, Activation: logistic,  Optimizer: adam, Reg Para: 0.1, Learning Rate: 0.01, accuracy: 0.9638, Absolute Error: 0.1411, Mean Squared Error: 0.7029 \n",
            "\n",
            "48) Model: NN, Parameters: No. hidden layers: 100, Activation: logistic,  Optimizer: adam, Reg Para: 0.1, Learning Rate: 0.1, accuracy: 0.8955, Absolute Error: 0.3851, Mean Squared Error: 1.8103 \n",
            "\n",
            "49) Model: NN, Parameters: No. hidden layers: 100, Activation: tanh,  Optimizer: sgd, Reg Para: 0.001, Learning Rate: 0.001, accuracy: 0.9449, Absolute Error: 0.2104, Mean Squared Error: 1.0284 \n",
            "\n",
            "50) Model: NN, Parameters: No. hidden layers: 100, Activation: tanh,  Optimizer: sgd, Reg Para: 0.001, Learning Rate: 0.01, accuracy: 0.9769, Absolute Error: 0.0916, Mean Squared Error: 0.4586 \n",
            "\n",
            "51) Model: NN, Parameters: No. hidden layers: 100, Activation: tanh,  Optimizer: sgd, Reg Para: 0.001, Learning Rate: 0.1, accuracy: 0.9796, Absolute Error: 0.0796, Mean Squared Error: 0.3902 \n",
            "\n",
            "52) Model: NN, Parameters: No. hidden layers: 100, Activation: tanh,  Optimizer: sgd, Reg Para: 0.1, Learning Rate: 0.001, accuracy: 0.9432, Absolute Error: 0.2178, Mean Squared Error: 1.0684 \n",
            "\n",
            "53) Model: NN, Parameters: No. hidden layers: 100, Activation: tanh,  Optimizer: sgd, Reg Para: 0.1, Learning Rate: 0.01, accuracy: 0.9759, Absolute Error: 0.095, Mean Squared Error: 0.4724 \n",
            "\n",
            "54) Model: NN, Parameters: No. hidden layers: 100, Activation: tanh,  Optimizer: sgd, Reg Para: 0.1, Learning Rate: 0.1, accuracy: 0.9793, Absolute Error: 0.0852, Mean Squared Error: 0.4446 \n",
            "\n",
            "55) Model: NN, Parameters: No. hidden layers: 100, Activation: tanh,  Optimizer: adam, Reg Para: 0.001, Learning Rate: 0.001, accuracy: 0.9791, Absolute Error: 0.0831, Mean Squared Error: 0.4269 \n",
            "\n",
            "56) Model: NN, Parameters: No. hidden layers: 100, Activation: tanh,  Optimizer: adam, Reg Para: 0.001, Learning Rate: 0.01, accuracy: 0.9713, Absolute Error: 0.1061, Mean Squared Error: 0.5185 \n",
            "\n",
            "57) Model: NN, Parameters: No. hidden layers: 100, Activation: tanh,  Optimizer: adam, Reg Para: 0.001, Learning Rate: 0.1, accuracy: 0.8665, Absolute Error: 0.5062, Mean Squared Error: 2.4822 \n",
            "\n",
            "58) Model: NN, Parameters: No. hidden layers: 100, Activation: tanh,  Optimizer: adam, Reg Para: 0.1, Learning Rate: 0.001, accuracy: 0.9785, Absolute Error: 0.0876, Mean Squared Error: 0.454 \n",
            "\n",
            "59) Model: NN, Parameters: No. hidden layers: 100, Activation: tanh,  Optimizer: adam, Reg Para: 0.1, Learning Rate: 0.01, accuracy: 0.9674, Absolute Error: 0.1289, Mean Squared Error: 0.6699 \n",
            "\n",
            "60) Model: NN, Parameters: No. hidden layers: 100, Activation: tanh,  Optimizer: adam, Reg Para: 0.1, Learning Rate: 0.1, accuracy: 0.8026, Absolute Error: 0.6766, Mean Squared Error: 2.9618 \n",
            "\n",
            "61) Model: NN, Parameters: No. hidden layers: 100, Activation: relu,  Optimizer: sgd, Reg Para: 0.001, Learning Rate: 0.001, accuracy: 0.9539, Absolute Error: 0.1782, Mean Squared Error: 0.8906 \n",
            "\n",
            "62) Model: NN, Parameters: No. hidden layers: 100, Activation: relu,  Optimizer: sgd, Reg Para: 0.001, Learning Rate: 0.01, accuracy: 0.9771, Absolute Error: 0.0884, Mean Squared Error: 0.4444 \n",
            "\n",
            "63) Model: NN, Parameters: No. hidden layers: 100, Activation: relu,  Optimizer: sgd, Reg Para: 0.001, Learning Rate: 0.1, accuracy: 0.9796, Absolute Error: 0.0794, Mean Squared Error: 0.4012 \n",
            "\n",
            "64) Model: NN, Parameters: No. hidden layers: 100, Activation: relu,  Optimizer: sgd, Reg Para: 0.1, Learning Rate: 0.001, accuracy: 0.9522, Absolute Error: 0.1838, Mean Squared Error: 0.9158 \n",
            "\n",
            "65) Model: NN, Parameters: No. hidden layers: 100, Activation: relu,  Optimizer: sgd, Reg Para: 0.1, Learning Rate: 0.01, accuracy: 0.9775, Absolute Error: 0.0909, Mean Squared Error: 0.4617 \n",
            "\n",
            "66) Model: NN, Parameters: No. hidden layers: 100, Activation: relu,  Optimizer: sgd, Reg Para: 0.1, Learning Rate: 0.1, accuracy: 0.9804, Absolute Error: 0.0805, Mean Squared Error: 0.4183 \n",
            "\n",
            "67) Model: NN, Parameters: No. hidden layers: 100, Activation: relu,  Optimizer: adam, Reg Para: 0.001, Learning Rate: 0.001, accuracy: 0.98, Absolute Error: 0.0784, Mean Squared Error: 0.3914 \n",
            "\n",
            "68) Model: NN, Parameters: No. hidden layers: 100, Activation: relu,  Optimizer: adam, Reg Para: 0.001, Learning Rate: 0.01, accuracy: 0.9726, Absolute Error: 0.1024, Mean Squared Error: 0.5042 \n",
            "\n",
            "69) Model: NN, Parameters: No. hidden layers: 100, Activation: relu,  Optimizer: adam, Reg Para: 0.001, Learning Rate: 0.1, accuracy: 0.8994, Absolute Error: 0.3958, Mean Squared Error: 1.9856 \n",
            "\n",
            "70) Model: NN, Parameters: No. hidden layers: 100, Activation: relu,  Optimizer: adam, Reg Para: 0.1, Learning Rate: 0.001, accuracy: 0.9797, Absolute Error: 0.0812, Mean Squared Error: 0.419 \n",
            "\n",
            "71) Model: NN, Parameters: No. hidden layers: 100, Activation: relu,  Optimizer: adam, Reg Para: 0.1, Learning Rate: 0.01, accuracy: 0.9679, Absolute Error: 0.1184, Mean Squared Error: 0.5768 \n",
            "\n",
            "72) Model: NN, Parameters: No. hidden layers: 100, Activation: relu,  Optimizer: adam, Reg Para: 0.1, Learning Rate: 0.1, accuracy: 0.8898, Absolute Error: 0.412, Mean Squared Error: 1.9904 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>K-NEAREST NEIGHBOUR</h1>"
      ],
      "metadata": {
        "id": "OZ9uxLv3JAB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_neighbors = [1, 3, 5, 7, 9]\n",
        "weights = [\"uniform\", \"distance\"]\n",
        "# algorithm = [\"ball_tree\", \"kd_tree\", \"brute\"]\n",
        "algorithm = [\"auto\"]\n",
        "\n",
        "KNN_models = dict()\n",
        "for n in n_neighbors:\n",
        "    for w in weights:\n",
        "        for a in algorithm:\n",
        "            model = KNeighborsClassifier(n_neighbors=n, weights=w, algorithm=a)\n",
        "            model.fit(X_train, y_train)\n",
        "            KNN_models[str(n)+w+a] = model\n",
        "            print(n, w, a)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCDgTQsTJD8y",
        "outputId": "4b1f5f3a-4465-4640-b39c-e7bfa1cb62de"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 uniform auto\n",
            "1 distance auto\n",
            "3 uniform auto\n",
            "3 distance auto\n",
            "5 uniform auto\n",
            "5 distance auto\n",
            "7 uniform auto\n",
            "7 distance auto\n",
            "9 uniform auto\n",
            "9 distance auto\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# predict\n",
        "n_neighbors = [1, 3, 5, 7, 9]\n",
        "weights = [\"uniform\", \"distance\"]\n",
        "# algorithm = [\"ball_tree\", \"kd_tree\", \"brute\"]\n",
        "algorithm = [\"auto\"]\n",
        "\n",
        "\n",
        "X_test_new = X_test[:1000]\n",
        "KNN_predicts = dict()\n",
        "for n in n_neighbors:\n",
        "    for w in weights:\n",
        "        for a in algorithm:\n",
        "            KNN_predicts[str(n)+w+a] = KNN_models[str(n)+w+a].predict(X_test_new)\n",
        "            print(str(n)+w+a)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-gkOPaB9iPg",
        "outputId": "f04f29a7-683a-4b81-e8f0-b1fb82e0427b"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1uniformauto\n",
            "1distanceauto\n",
            "3uniformauto\n",
            "3distanceauto\n",
            "5uniformauto\n",
            "5distanceauto\n",
            "7uniformauto\n",
            "7distanceauto\n",
            "9uniformauto\n",
            "9distanceauto\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "for n in n_neighbors:\n",
        "    for w in weights:\n",
        "        for a in algorithm:\n",
        "            # accuracy\n",
        "            i += 1\n",
        "            print(f\"{i}) Model: KNN, Parameters: Nearest Neighbours: {n}, Weight Function: {w},  Algorithm: {a}, accuracy: {metrics.accuracy_score(y_true=y_test[:1000], y_pred=KNN_predicts[str(n)+w+a])}, Absolute Error: {metrics.mean_absolute_error(y_true=y_test[:1000], y_pred=KNN_predicts[str(n)+w+a])}, Mean Squared Error: {metrics.mean_squared_error(y_true=y_test[:1000], y_pred=KNN_predicts[str(n)+w+a])} \\n\")\n",
        "\n",
        "            # cm\n",
        "            # print(metrics.confusion_matrix(y_true=y_test[:1000], y_pred=KNN_predicts[str(n)+w+a]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jupOYQkLB2QD",
        "outputId": "640edc71-cb7a-436d-b5a9-27d61a902529"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1) Model: KNN, Parameters: Nearest Neighbours: 3, Weight Function: uniform,  Algorithm: ball_tree, accuracy: 0.962, Absolute Error: 0.149, Mean Squared Error: 0.715 \n",
            "\n",
            "2) Model: KNN, Parameters: Nearest Neighbours: 3, Weight Function: uniform,  Algorithm: kd_tree, accuracy: 0.962, Absolute Error: 0.149, Mean Squared Error: 0.715 \n",
            "\n",
            "3) Model: KNN, Parameters: Nearest Neighbours: 3, Weight Function: uniform,  Algorithm: brute, accuracy: 0.962, Absolute Error: 0.149, Mean Squared Error: 0.715 \n",
            "\n",
            "4) Model: KNN, Parameters: Nearest Neighbours: 3, Weight Function: distance,  Algorithm: ball_tree, accuracy: 0.965, Absolute Error: 0.139, Mean Squared Error: 0.661 \n",
            "\n",
            "5) Model: KNN, Parameters: Nearest Neighbours: 3, Weight Function: distance,  Algorithm: kd_tree, accuracy: 0.965, Absolute Error: 0.139, Mean Squared Error: 0.661 \n",
            "\n",
            "6) Model: KNN, Parameters: Nearest Neighbours: 3, Weight Function: distance,  Algorithm: brute, accuracy: 0.965, Absolute Error: 0.139, Mean Squared Error: 0.661 \n",
            "\n",
            "7) Model: KNN, Parameters: Nearest Neighbours: 5, Weight Function: uniform,  Algorithm: ball_tree, accuracy: 0.961, Absolute Error: 0.154, Mean Squared Error: 0.734 \n",
            "\n",
            "8) Model: KNN, Parameters: Nearest Neighbours: 5, Weight Function: uniform,  Algorithm: kd_tree, accuracy: 0.961, Absolute Error: 0.154, Mean Squared Error: 0.734 \n",
            "\n",
            "9) Model: KNN, Parameters: Nearest Neighbours: 5, Weight Function: uniform,  Algorithm: brute, accuracy: 0.961, Absolute Error: 0.154, Mean Squared Error: 0.734 \n",
            "\n",
            "10) Model: KNN, Parameters: Nearest Neighbours: 5, Weight Function: distance,  Algorithm: ball_tree, accuracy: 0.964, Absolute Error: 0.144, Mean Squared Error: 0.69 \n",
            "\n",
            "11) Model: KNN, Parameters: Nearest Neighbours: 5, Weight Function: distance,  Algorithm: kd_tree, accuracy: 0.964, Absolute Error: 0.144, Mean Squared Error: 0.69 \n",
            "\n",
            "12) Model: KNN, Parameters: Nearest Neighbours: 5, Weight Function: distance,  Algorithm: brute, accuracy: 0.964, Absolute Error: 0.144, Mean Squared Error: 0.69 \n",
            "\n",
            "13) Model: KNN, Parameters: Nearest Neighbours: 7, Weight Function: uniform,  Algorithm: ball_tree, accuracy: 0.962, Absolute Error: 0.153, Mean Squared Error: 0.749 \n",
            "\n",
            "14) Model: KNN, Parameters: Nearest Neighbours: 7, Weight Function: uniform,  Algorithm: kd_tree, accuracy: 0.962, Absolute Error: 0.153, Mean Squared Error: 0.749 \n",
            "\n",
            "15) Model: KNN, Parameters: Nearest Neighbours: 7, Weight Function: uniform,  Algorithm: brute, accuracy: 0.962, Absolute Error: 0.153, Mean Squared Error: 0.749 \n",
            "\n",
            "16) Model: KNN, Parameters: Nearest Neighbours: 7, Weight Function: distance,  Algorithm: ball_tree, accuracy: 0.964, Absolute Error: 0.142, Mean Squared Error: 0.686 \n",
            "\n",
            "17) Model: KNN, Parameters: Nearest Neighbours: 7, Weight Function: distance,  Algorithm: kd_tree, accuracy: 0.964, Absolute Error: 0.142, Mean Squared Error: 0.686 \n",
            "\n",
            "18) Model: KNN, Parameters: Nearest Neighbours: 7, Weight Function: distance,  Algorithm: brute, accuracy: 0.964, Absolute Error: 0.142, Mean Squared Error: 0.686 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "for n in n_neighbors:\n",
        "    for w in weights:\n",
        "        for a in algorithm:\n",
        "            # accuracy\n",
        "            i += 1\n",
        "            print(f\"{i}) Model: KNN, Parameters: Nearest Neighbours: {n}, Weight Function: {w},  Algorithm: {a}, accuracy: {metrics.accuracy_score(y_true=y_test[:1000], y_pred=KNN_predicts[str(n)+w+a])}, Absolute Error: {metrics.mean_absolute_error(y_true=y_test[:1000], y_pred=KNN_predicts[str(n)+w+a])}, Mean Squared Error: {metrics.mean_squared_error(y_true=y_test[:1000], y_pred=KNN_predicts[str(n)+w+a])} \\n\")\n",
        "\n",
        "            # cm\n",
        "            # print(metrics.confusion_matrix(y_true=y_test[:1000], y_pred=KNN_predicts[str(n)+w+a]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2gVwDdlKNLI",
        "outputId": "ddc38f21-a1dc-499e-e68e-da5ec76944ed"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1) Model: KNN, Parameters: Nearest Neighbours: 1, Weight Function: uniform,  Algorithm: auto, accuracy: 0.962, Absolute Error: 0.142, Mean Squared Error: 0.65 \n",
            "\n",
            "2) Model: KNN, Parameters: Nearest Neighbours: 1, Weight Function: distance,  Algorithm: auto, accuracy: 0.962, Absolute Error: 0.142, Mean Squared Error: 0.65 \n",
            "\n",
            "3) Model: KNN, Parameters: Nearest Neighbours: 3, Weight Function: uniform,  Algorithm: auto, accuracy: 0.962, Absolute Error: 0.149, Mean Squared Error: 0.715 \n",
            "\n",
            "4) Model: KNN, Parameters: Nearest Neighbours: 3, Weight Function: distance,  Algorithm: auto, accuracy: 0.965, Absolute Error: 0.139, Mean Squared Error: 0.661 \n",
            "\n",
            "5) Model: KNN, Parameters: Nearest Neighbours: 5, Weight Function: uniform,  Algorithm: auto, accuracy: 0.961, Absolute Error: 0.154, Mean Squared Error: 0.734 \n",
            "\n",
            "6) Model: KNN, Parameters: Nearest Neighbours: 5, Weight Function: distance,  Algorithm: auto, accuracy: 0.964, Absolute Error: 0.144, Mean Squared Error: 0.69 \n",
            "\n",
            "7) Model: KNN, Parameters: Nearest Neighbours: 7, Weight Function: uniform,  Algorithm: auto, accuracy: 0.962, Absolute Error: 0.153, Mean Squared Error: 0.749 \n",
            "\n",
            "8) Model: KNN, Parameters: Nearest Neighbours: 7, Weight Function: distance,  Algorithm: auto, accuracy: 0.964, Absolute Error: 0.142, Mean Squared Error: 0.686 \n",
            "\n",
            "9) Model: KNN, Parameters: Nearest Neighbours: 9, Weight Function: uniform,  Algorithm: auto, accuracy: 0.952, Absolute Error: 0.188, Mean Squared Error: 0.902 \n",
            "\n",
            "10) Model: KNN, Parameters: Nearest Neighbours: 9, Weight Function: distance,  Algorithm: auto, accuracy: 0.957, Absolute Error: 0.17, Mean Squared Error: 0.824 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MjO6_iX6KNdr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}