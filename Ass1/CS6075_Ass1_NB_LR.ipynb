{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import nltk.corpus\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import unicodedata\n",
        "import math\n",
        "import copy\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from zipfile import ZipFile\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jN_nMNVO8jL",
        "outputId": "abb4991f-99fd-48ba-f6c5-fb233ea920aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "stop = stopwords.words('english')"
      ],
      "metadata": {
        "id": "JcApFhUJUtji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>EVALUATION OF MODELS</h1>"
      ],
      "metadata": {
        "id": "Ef5FehSkoeHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_evaluation_metrics(Y, Y_pred):\n",
        "    conf = confusion_matrix(y_true = Y, y_pred=Y_pred)\n",
        "    TN, FP, FN, TP = conf.ravel()\n",
        "\n",
        "    accuracy = (TN + TP)  / (TN + FP + FN + TP)\n",
        "\n",
        "    precision = TP / (TP + FP)\n",
        "\n",
        "    recall = TP / (FN + TP)\n",
        "\n",
        "    f1 = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "    return accuracy, precision, recall, f1"
      ],
      "metadata": {
        "id": "6yGaNv2FpFN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# metrics_dict = {\"train\": {\"hw1\": [], \"enron1\": [], \"enron4\": []}, \"test\": {\"hw1\": [], \"enron1\": [], \"enron4\": []}}\n",
        "\n",
        "def print_metrics_dict(metrics_dict):\n",
        "    s = \"\"\n",
        "    for t in metrics_dict:\n",
        "        for k in metrics_dict[t]:\n",
        "            s += f'Set: {t}, Dataset: {k} -> Accuracy: {metrics_dict[t][k][0]}, Precision: {metrics_dict[t][k][1]}, Recall: {metrics_dict[t][k][2]}, F1-Score: {metrics_dict[t][k][3]}\\n'\n",
        "            \n",
        "    return s"
      ],
      "metadata": {
        "id": "NCOD3cmMQVC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>FILE HANDLING</h1>\n",
        "1. Run the below cell to create a folder named: rhugaved_data\n",
        "2. Upload zip files of dataset to the folder\n"
      ],
      "metadata": {
        "id": "H0oTCTa8SorY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.mkdir(\"/content/rhugaved_data\")\n",
        "os.chdir(\"/content/rhugaved_data\")"
      ],
      "metadata": {
        "id": "-mQfBFxrSoXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run below cell to unzip the files"
      ],
      "metadata": {
        "id": "_LoSKbmvTAgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for filename in os.listdir(\"/content/rhugaved_data\"):\n",
        "    if not os.path.isdir(filename):\n",
        "        with ZipFile(filename, 'r') as zip:\n",
        "            # extracting all the files\n",
        "            zip.extractall()"
      ],
      "metadata": {
        "id": "PMgh2NLnS_JM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting the folders containing the ham and spam datasets of different datasets"
      ],
      "metadata": {
        "id": "C6-svCB0UKg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_folders = {\"hw1\": {\"train\": [], \"test\": []}, \n",
        "                   \"enron1\": {\"train\": [], \"test\": []}, \n",
        "                   \"enron4\": {\"train\": [], \"test\": []}\n",
        "                   }\n",
        "\n",
        "base_folders_to_filter = [\"train\", \"test\", \"enron1\", \"enron4\"]\n",
        "def get_dataset_folder_paths(dataset_folders):\n",
        "    for filename in os.listdir(\"/content/rhugaved_data\"):\n",
        "        folders = [os.path.abspath(filename) for filename in os.listdir(\".\") if os.path.isdir(filename) and filename in base_folders_to_filter]\n",
        "\n",
        "    # Add train and test folders to enron\n",
        "    copy = folders.copy()\n",
        "    for i, f in enumerate(copy):\n",
        "        if \"enron\" in f:\n",
        "            folders.append(folders[i] + \"/test\")\n",
        "            folders[i] += \"/train\"\n",
        "            \n",
        "    # print(folders)\n",
        "\n",
        "    for i in range(len(folders)):\n",
        "        folders.append(folders[i] + \"/spam\")\n",
        "        folders[i] += \"/ham\"\n",
        "    # print((folders))\n",
        "\n",
        "    for i in folders:\n",
        "        flag = 0\n",
        "        for k in dataset_folders:\n",
        "            if k in i:\n",
        "                if \"train\" in i:\n",
        "                    dataset_folders[k][\"train\"].append(i)\n",
        "                else:\n",
        "                    dataset_folders[k][\"test\"].append(i)\n",
        "                flag = 1\n",
        "        if not flag:\n",
        "            if \"train\" in i:\n",
        "                    dataset_folders[\"hw1\"][\"train\"].append(i)\n",
        "            else:\n",
        "                dataset_folders[\"hw1\"][\"test\"].append(i)\n",
        "            \n",
        "\n",
        "    print(dataset_folders)\n",
        "    return dataset_folders\n",
        "dataset_folders = get_dataset_folder_paths(dataset_folders)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SO5Fa_vpt1q",
        "outputId": "ff1bbf12-1cce-41ab-c4df-ba1b1e5a6f81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'hw1': {'train': ['/content/rhugaved_data/train/ham', '/content/rhugaved_data/train/spam'], 'test': ['/content/rhugaved_data/test/ham', '/content/rhugaved_data/test/spam']}, 'enron1': {'train': ['/content/rhugaved_data/enron1/train/ham', '/content/rhugaved_data/enron1/train/spam'], 'test': ['/content/rhugaved_data/enron1/test/ham', '/content/rhugaved_data/enron1/test/spam']}, 'enron4': {'train': ['/content/rhugaved_data/enron4/train/ham', '/content/rhugaved_data/enron4/train/spam'], 'test': ['/content/rhugaved_data/enron4/test/ham', '/content/rhugaved_data/enron4/test/spam']}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making a copy of the structure of dataset_folders dict. We will store the actual text data in data as dataframes"
      ],
      "metadata": {
        "id": "R5PyBygr7zPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = copy.deepcopy(dataset_folders)\n",
        "lr_data = copy.deepcopy(dataset_folders)\n",
        "\n",
        "for k in data:\n",
        "    for t in data[k]:\n",
        "        data[k][t] = {\"ham\": dict(), \"spam\": dict()}\n",
        "        lr_data[k][t] = dict()\n",
        "\n",
        "dataset_folders"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BhHE4Lk4q4f",
        "outputId": "9f1dce32-3a6a-41ff-8975-4f835fa369f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'hw1': {'train': ['/content/rhugaved_data/train/ham',\n",
              "   '/content/rhugaved_data/train/spam'],\n",
              "  'test': ['/content/rhugaved_data/test/ham',\n",
              "   '/content/rhugaved_data/test/spam']},\n",
              " 'enron1': {'train': ['/content/rhugaved_data/enron1/train/ham',\n",
              "   '/content/rhugaved_data/enron1/train/spam'],\n",
              "  'test': ['/content/rhugaved_data/enron1/test/ham',\n",
              "   '/content/rhugaved_data/enron1/test/spam']},\n",
              " 'enron4': {'train': ['/content/rhugaved_data/enron4/train/ham',\n",
              "   '/content/rhugaved_data/enron4/train/spam'],\n",
              "  'test': ['/content/rhugaved_data/enron4/test/ham',\n",
              "   '/content/rhugaved_data/enron4/test/spam']}}"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Tzr0d7dE10U",
        "outputId": "1af0eb83-f7e6-4d3a-b21a-a0a5a64f8af7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'hw1': {'train': {}, 'test': {}},\n",
              " 'enron1': {'train': {}, 'test': {}},\n",
              " 'enron4': {'train': {}, 'test': {}}}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text_from_dataset(dataset_folders, data):\n",
        "    \n",
        "    for k in dataset_folders:\n",
        "        for t in dataset_folders[k]:\n",
        "            for hs in dataset_folders[k][t]:\n",
        "                all_text = []\n",
        "                for filename in os.listdir(hs):\n",
        "                    with open(hs + \"/\" + filename, 'r', encoding=\"latin-1\") as text:\n",
        "                        all_text.append(text.read())\n",
        "                        # print(all_text)\n",
        "                        # return\n",
        "                if \"ham\" in filename:\n",
        "                    data[k][t]['ham'][\"X\"] = all_text\n",
        "                    data[k][t]['ham'][\"Y\"] = [0] * len(all_text)\n",
        "                else:\n",
        "                    data[k][t]['spam'][\"X\"] = all_text\n",
        "                    data[k][t]['spam'][\"Y\"] = [1] * len(all_text)\n",
        "get_text_from_dataset(dataset_folders, data)"
      ],
      "metadata": {
        "id": "ylpimXHlqcxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Cleaning of Text Data:</h1>\n",
        "1. Normalizing i.e lower case all letters\n",
        "2. Remove unicode characters and numbers\n",
        "3. Remove links and emails\n",
        "3. Remove Stop words\n"
      ],
      "metadata": {
        "id": "NCtmaD23OVur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "METHOD TO CLEAN TEXT PER DOCUMENT"
      ],
      "metadata": {
        "id": "2sXf2KkDjrF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text_document(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"(\\n)\", \" \", text)\n",
        "    text = re.sub(r\"(@\\[A-Za-z0-9]+)|([^A-Za-z ])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", text)\n",
        "    text = re.sub(r\"(\\t)|( +)\", \" \", text)\n",
        "    text = \" \".join([word for word in text.split() if word not in (stop) and len(word) > 2])\n",
        "    words_list = text.split()\n",
        "    return text, words_list"
      ],
      "metadata": {
        "id": "69vkMtN_jqi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>VOCAB BUILDER</h1>"
      ],
      "metadata": {
        "id": "vtoXYkDoA3Pf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vocab_builder(all_text):\n",
        "    vocab = []\n",
        "    for text in all_text:\n",
        "        _, words_list = clean_text_document(text)\n",
        "        vocab.extend(words_list)\n",
        "    return sorted(set(vocab))"
      ],
      "metadata": {
        "id": "BneTAcijN9Vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Bag of Words</h1>\n",
        "and\n",
        "<h1>Bernoulli</h1>"
      ],
      "metadata": {
        "id": "CXcrmq-3o06I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def bow_builder(local_all_text, local_vocab):\n",
        "    bag_of_words_corpus = []\n",
        "    bag_of_words = np.zeros(len(local_vocab))  \n",
        "    for text in local_all_text:        \n",
        "        _, words_list = clean_text_document(text)   \n",
        "                \n",
        "        for word in words_list:            \n",
        "            for i, w in enumerate(local_vocab):                \n",
        "                if word == w:                     \n",
        "                    bag_of_words[i] += 1  \n",
        "                    \n",
        "    return bag_of_words\n"
      ],
      "metadata": {
        "id": "AMiuOYedi7K2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2FUsHsBH6YtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Multinomial NB</h1>"
      ],
      "metadata": {
        "id": "YG-GX_LWpReC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Contional Probabilites"
      ],
      "metadata": {
        "id": "18i1ZNxXIQaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cond_prob_multi(class_bow, class_all_text, vocab):\n",
        "    cond_prob = []\n",
        "    length = 0\n",
        "    for l in class_all_text:\n",
        "        length += len(l)\n",
        "    for i, w in enumerate(class_bow):\n",
        "        p = (w + 1) /(length + len(vocab))\n",
        "        cond_prob.append(p)\n",
        "    return cond_prob"
      ],
      "metadata": {
        "id": "KhMN5jDmRqNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_multi_NB(class_prior, cond_prob, vocab, doc):\n",
        "    bag_of_words_doc = bow_builder([doc], vocab)\n",
        "    # for class ham\n",
        "    prob_ham = math.log(class_prior[0])\n",
        "    for i, p in enumerate(cond_prob[0]):\n",
        "        prob_ham += math.log(p) * bag_of_words_doc[i]\n",
        "\n",
        "    # for class spam\n",
        "    prob_spam = math.log(class_prior[1])\n",
        "    for i, p in enumerate(cond_prob[1]):\n",
        "        prob_spam += math.log(p) * bag_of_words_doc[i]\n",
        "\n",
        "    return prob_ham, prob_spam"
      ],
      "metadata": {
        "id": "fmr8vYBmK5nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Making a function which when called will run mulitNB on all datasets and return the predictions</h3>"
      ],
      "metadata": {
        "id": "07q6wfz0h5yi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_dict = {\"hw1\": None, \"enron1\": None, \"enron4\": None}\n",
        "bag_of_words_dict = {\"ham\": {\"hw1\": None, \"enron1\": None, \"enron4\": None}, \"spam\": {\"hw1\": None, \"enron1\": None, \"enron4\": None}}\n",
        "prior_dict = {\"ham\": {\"hw1\": None, \"enron1\": None, \"enron4\": None}, \"spam\": {\"hw1\": None, \"enron1\": None, \"enron4\": None}}\n",
        "multi_cond_prob_dict = {\"ham\": {\"hw1\": None, \"enron1\": None, \"enron4\": None}, \"spam\": {\"hw1\": None, \"enron1\": None, \"enron4\": None}}\n",
        "Y_pred_dict = {\"train\": {\"hw1\": [], \"enron1\": [], \"enron4\": []}, \"test\": {\"hw1\": [], \"enron1\": [], \"enron4\": []}}\n",
        "metrics_dict = {\"train\": {\"hw1\": [], \"enron1\": [], \"enron4\": []}, \"test\": {\"hw1\": [], \"enron1\": [], \"enron4\": []}}\n",
        "def multi_NB(data):\n",
        "    for k in data:\n",
        "        print(\"Dataset: \", k)\n",
        "        # Assign X and Y and spam and ham texts\n",
        "        ham_all_text = data[k]['train']['ham']['X']\n",
        "        spam_all_text = data[k]['train']['spam']['X']\n",
        "        X = ham_all_text + spam_all_text\n",
        "        Y = data[k]['train']['ham']['Y'] + data[k]['train']['spam']['Y']\n",
        "\n",
        "        # Call vocab builder and save it in vocab_dict\n",
        "        vocab_dict[k] = vocab_builder(X)\n",
        "        # print(len(vocab_dict[k]))\n",
        "\n",
        "        # Make bow and save respective ham and spam in the dict\n",
        "        bag_of_words_dict[\"ham\"][k] = bow_builder(ham_all_text, vocab_dict[k])\n",
        "        bag_of_words_dict[\"spam\"][k] = bow_builder(spam_all_text, vocab_dict[k])\n",
        "\n",
        "        # calculate priors for each dataset and save in dict\n",
        "        prior_dict[\"ham\"][k] = len(ham_all_text) / (len(ham_all_text) + len(spam_all_text))\n",
        "        prior_dict[\"spam\"][k] = len(spam_all_text) / (len(ham_all_text) + len(spam_all_text))\n",
        "\n",
        "        # calculate conditional probablity and store in dict\n",
        "        multi_cond_prob_dict[\"ham\"][k] = get_cond_prob_multi(bag_of_words_dict[\"ham\"][k], ham_all_text, vocab_dict[k])\n",
        "        multi_cond_prob_dict[\"spam\"][k] = get_cond_prob_multi(bag_of_words_dict[\"spam\"][k], spam_all_text, vocab_dict[k])\n",
        "        \n",
        "        \n",
        "        correct = 0\n",
        "        # calculate the training set metrics\n",
        "        for i, doc in enumerate(X):\n",
        "            p = apply_multi_NB([prior_dict[\"ham\"][k], prior_dict[\"spam\"][k]], [multi_cond_prob_dict[\"ham\"][k], \n",
        "                                multi_cond_prob_dict[\"spam\"][k]], vocab_dict[k], doc)\n",
        "            \n",
        "            if p[0] > p[1]:\n",
        "                Y_pred_dict[\"train\"][k].append(0)\n",
        "                if Y[i] == 0:\n",
        "                    correct += 1\n",
        "\n",
        "            elif p[0] < p[1]:\n",
        "                Y_pred_dict[\"train\"][k].append(1)\n",
        "                if Y[i] == 1:\n",
        "                    correct += 1\n",
        "        # print(correct, len(Y), len(Y_pred_dict[\"train\"][k]))\n",
        "        print(f'Correctly predicted: {correct}, total train examples: {len(Y)}, total predicted examples: {len(Y_pred_dict[\"train\"][k])}')        \n",
        "\n",
        "        # Evalute on test data\n",
        "        X_test = data[k]['test']['ham']['X'] + data[k]['test']['spam']['X']\n",
        "        Y_test = data[k]['test']['ham']['Y'] + data[k]['test']['spam']['Y']\n",
        "        test_correct = 0\n",
        "        for i, doc in enumerate(X_test):\n",
        "            p = apply_multi_NB([prior_dict[\"ham\"][k], prior_dict[\"spam\"][k]], [multi_cond_prob_dict[\"ham\"][k], \n",
        "                                multi_cond_prob_dict[\"spam\"][k]], vocab_dict[k], doc)\n",
        "            \n",
        "            if p[0] > p[1]:\n",
        "                Y_pred_dict[\"test\"][k].append(0)\n",
        "                if Y_test[i] == 0:\n",
        "                    test_correct += 1\n",
        "\n",
        "            elif p[0] < p[1]:\n",
        "                Y_pred_dict[\"test\"][k].append(1)\n",
        "                if Y_test[i] == 1:\n",
        "                    test_correct += 1\n",
        "        print(f'Correctly predicted: {test_correct}, total test examples: {len(Y_test)}, total predicted examples: {len(Y_pred_dict[\"test\"][k])}')        \n",
        "        # metrics_dict[\"train\"][k] = get_evaluation_metrics(Y, Y_pred_dict[\"train\"][k])\n",
        "        \n",
        "\n",
        "        \n",
        "\n",
        "multi_NB(data)"
      ],
      "metadata": {
        "id": "wyu7YVxDHA0s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "146dc1a3-d9c5-4e4d-a58f-caca5e941ad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset:  hw1\n",
            "Correctly predicted: 456, total test examples: 463, total predicted examples: 463\n",
            "Correctly predicted: 451, total test examples: 478, total predicted examples: 478\n",
            "Dataset:  enron1\n",
            "Correctly predicted: 447, total test examples: 450, total predicted examples: 450\n",
            "Correctly predicted: 428, total test examples: 456, total predicted examples: 456\n",
            "Dataset:  enron4\n",
            "Correctly predicted: 417, total test examples: 535, total predicted examples: 535\n",
            "Correctly predicted: 466, total test examples: 543, total predicted examples: 543\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CCjptZ9OO9nl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k in data:\n",
        "    # evalution on training data\n",
        "    # print(k)\n",
        "    Y = data[k]['train']['ham']['Y'] + data[k]['train']['spam']['Y']\n",
        "    # print(np.shape(Y))\n",
        "    # print(np.shape(Y_pred_dict[\"train\"][k]))\n",
        "    metrics_dict[\"train\"][k] = get_evaluation_metrics(Y, Y_pred_dict[\"train\"][k])\n",
        "\n",
        "    # evaluation on testing data\n",
        "    Y = data[k]['test']['ham']['Y'] + data[k]['test']['spam']['Y']\n",
        "    # print(np.shape(Y))\n",
        "    # print(np.shape(Y_pred_dict[\"test\"][k]))\n",
        "    metrics_dict[\"test\"][k] = get_evaluation_metrics(Y, Y_pred_dict[\"test\"][k])\n",
        "print(print_metrics_dict(metrics_dict))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZMoh8aUylA-",
        "outputId": "525bb4d5-85a0-404e-d680-b6861be507ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Set: train, Dataset: hw1 -> Accuracy: 0.9848812095032398, Precision: 0.953125, Recall: 0.991869918699187, F1-Score: 0.9721115537848606\n",
            "Set: train, Dataset: enron1 -> Accuracy: 0.9933333333333333, Precision: 0.9776119402985075, Recall: 1.0, F1-Score: 0.9886792452830189\n",
            "Set: train, Dataset: enron4 -> Accuracy: 0.7794392523364486, Precision: 1.0, Recall: 0.7064676616915423, F1-Score: 0.8279883381924198\n",
            "Set: test, Dataset: hw1 -> Accuracy: 0.9435146443514645, Precision: 0.8503401360544217, Recall: 0.9615384615384616, F1-Score: 0.9025270758122743\n",
            "Set: test, Dataset: enron1 -> Accuracy: 0.9385964912280702, Precision: 0.8903225806451613, Recall: 0.9261744966442953, F1-Score: 0.9078947368421052\n",
            "Set: test, Dataset: enron4 -> Accuracy: 0.858195211786372, Precision: 1.0, Recall: 0.80306905370844, F1-Score: 0.8907801418439717\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>BERNOULLI</h1>"
      ],
      "metadata": {
        "id": "xpbta0bBHBCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def bernoulli_builder(local_all_text, local_vocab):\n",
        "    # bernoulli = np.zeros(len(local_vocab))  \n",
        "    bernoulli_doc_occurances = np.zeros(len(local_vocab))  \n",
        "\n",
        "    for text in local_all_text:        \n",
        "        _, words_list = clean_text_document(text)   \n",
        "        # For bernoulli we can have just unique words for calculation\n",
        "        words_list = list(set(words_list))\n",
        "        for word in words_list:            \n",
        "            for i, w in enumerate(local_vocab):                \n",
        "                if word == w:                     \n",
        "                    # bernoulli[i] = 1  \n",
        "                    bernoulli_doc_occurances[i] = bernoulli_doc_occurances[i] + 1\n",
        "                    \n",
        "    return bernoulli_doc_occurances\n"
      ],
      "metadata": {
        "id": "Sg-sCxY3N56c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cond_prob_bernoulli(class_bernoulli_doc_occurances, class_all_text, vocab):\n",
        "    cond_prob = []\n",
        "    no_docs = len(class_all_text)\n",
        "    for i, w in enumerate(class_bernoulli_doc_occurances):\n",
        "        p = (w + 1) /(no_docs + 2)\n",
        "        cond_prob.append(p)\n",
        "    return cond_prob"
      ],
      "metadata": {
        "id": "19Jjeg9fHEST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_bernoulli_NB(class_prior, cond_prob, vocab, doc):\n",
        "    bernoulli_doc = bow_builder([doc], vocab)\n",
        "    # for class ham\n",
        "    prob_ham = math.log(class_prior[0])\n",
        "    # print(cond_prob[0])\n",
        "    for i, p in enumerate(cond_prob[0]):\n",
        "        if bernoulli_doc[i]:\n",
        "            prob_ham += math.log(p)\n",
        "        else:\n",
        "            q = 1 - p\n",
        "            prob_ham += math.log(q)\n",
        "\n",
        "    # for class spam\n",
        "    prob_spam = math.log(class_prior[1])\n",
        "    for i, p in enumerate(cond_prob[1]):\n",
        "        if bernoulli_doc[i]:\n",
        "            prob_spam += math.log(p)\n",
        "        else:\n",
        "            q = 1 - p\n",
        "            prob_spam += math.log(q)\n",
        "    return prob_ham, prob_spam"
      ],
      "metadata": {
        "id": "py4jv4BeZ7sA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primary function to apply bernoulli NB on all datasets"
      ],
      "metadata": {
        "id": "dAltiYQ1_xbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vocab_dict = {\"hw1\": None, \"enron1\": None, \"enron4\": None}\n",
        "bernoulli_dict = {\"ham\": {\"hw1\": None, \"enron1\": None, \"enron4\": None}, \"spam\": {\"hw1\": None, \"enron1\": None, \"enron4\": None}}\n",
        "# prior_dict = {\"ham\": {\"hw1\": None, \"enron1\": None, \"enron4\": None}, \"spam\": {\"hw1\": None, \"enron1\": None, \"enron4\": None}}\n",
        "bernoulli_cond_prob_dict = {\"ham\": {\"hw1\": None, \"enron1\": None, \"enron4\": None}, \"spam\": {\"hw1\": None, \"enron1\": None, \"enron4\": None}}\n",
        "bernoulli_Y_pred_dict = {\"train\": {\"hw1\": [], \"enron1\": [], \"enron4\": []}, \"test\": {\"hw1\": [], \"enron1\": [], \"enron4\": []}}\n",
        "bernoulli_metrics_dict = {\"train\": {\"hw1\": [], \"enron1\": [], \"enron4\": []}, \"test\": {\"hw1\": [], \"enron1\": [], \"enron4\": []}}\n",
        "def bernoulli_NB(data):\n",
        "    for k in data:\n",
        "        print(\"Dataset: \", k)\n",
        "        # Assign X and Y and spam and ham texts\n",
        "        ham_all_text = data[k]['train']['ham']['X']\n",
        "        spam_all_text = data[k]['train']['spam']['X']\n",
        "        X = ham_all_text + spam_all_text\n",
        "        Y = data[k]['train']['ham']['Y'] + data[k]['train']['spam']['Y']\n",
        "\n",
        "        # Call vocab builder and save it in vocab_dict\n",
        "        # vocab_dict[k] = vocab_builder(X)\n",
        "        # print(len(vocab_dict[k]))\n",
        "\n",
        "        # Make bow and save respective ham and spam in the dict\n",
        "        bernoulli_dict[\"ham\"][k] = bernoulli_builder(ham_all_text, vocab_dict[k])\n",
        "        bernoulli_dict[\"spam\"][k] = bernoulli_builder(spam_all_text, vocab_dict[k])\n",
        "\n",
        "        # calculate priors for each dataset and save in dict\n",
        "        # prior_dict[\"ham\"][k] = len(ham_all_text) / (len(ham_all_text) + len(spam_all_text))\n",
        "        # prior_dict[\"spam\"][k] = len(spam_all_text) / (len(ham_all_text) + len(spam_all_text))\n",
        "\n",
        "        # calculate conditional probablity and store in dict\n",
        "        bernoulli_cond_prob_dict[\"ham\"][k] = get_cond_prob_bernoulli(bernoulli_dict[\"ham\"][k], ham_all_text, vocab_dict[k])\n",
        "        bernoulli_cond_prob_dict[\"spam\"][k] = get_cond_prob_bernoulli(bernoulli_dict[\"spam\"][k], spam_all_text, vocab_dict[k])\n",
        "        \n",
        "        \n",
        "        correct = 0\n",
        "        # calculate the training set metrics\n",
        "        for i, doc in enumerate(X):\n",
        "            p = apply_bernoulli_NB([prior_dict[\"ham\"][k], prior_dict[\"spam\"][k]], [bernoulli_cond_prob_dict[\"ham\"][k], \n",
        "                                bernoulli_cond_prob_dict[\"spam\"][k]], vocab_dict[k], doc)\n",
        "            \n",
        "            if p[0] > p[1]:\n",
        "                bernoulli_Y_pred_dict[\"train\"][k].append(0)\n",
        "                if Y[i] == 0:\n",
        "                    correct += 1\n",
        "\n",
        "            elif p[0] < p[1]:\n",
        "                bernoulli_Y_pred_dict[\"train\"][k].append(1)\n",
        "                if Y[i] == 1:\n",
        "                    correct += 1\n",
        "        # print(correct, len(Y), len(bernoulli_Y_pred_dict[\"train\"][k]))\n",
        "        print(f'Correctly predicted: {correct}, total train examples: {len(Y)}, total predicted examples: {len(bernoulli_Y_pred_dict[\"train\"][k])}')        \n",
        "\n",
        "\n",
        "        # Evalute on test data\n",
        "        X_test = data[k]['test']['ham']['X'] + data[k]['test']['spam']['X']\n",
        "        Y_test = data[k]['test']['ham']['Y'] + data[k]['test']['spam']['Y']\n",
        "        test_correct = 0\n",
        "        for i, doc in enumerate(X_test):\n",
        "            p = apply_bernoulli_NB([prior_dict[\"ham\"][k], prior_dict[\"spam\"][k]], [bernoulli_cond_prob_dict[\"ham\"][k], \n",
        "                                bernoulli_cond_prob_dict[\"spam\"][k]], vocab_dict[k], doc)\n",
        "            \n",
        "            if p[0] > p[1]:\n",
        "                bernoulli_Y_pred_dict[\"test\"][k].append(0)\n",
        "                if Y_test[i] == 0:\n",
        "                    test_correct += 1\n",
        "\n",
        "            elif p[0] < p[1]:\n",
        "                bernoulli_Y_pred_dict[\"test\"][k].append(1)\n",
        "                if Y_test[i] == 1:\n",
        "                    test_correct += 1\n",
        "        # print(test_correct, len(Y_test), len(bernoulli_Y_pred_dict[\"test\"][k]))  \n",
        "        print(f'Correctly predicted: {test_correct}, total test examples: {len(Y_test)}, total predicted examples: {len(bernoulli_Y_pred_dict[\"test\"][k])}')        \n",
        "      \n",
        "        # metrics_dict[\"train\"][k] = get_evaluation_metrics(Y, Y_pred_dict[\"train\"][k])\n",
        "        \n",
        "\n",
        "\n",
        "bernoulli_NB(data)"
      ],
      "metadata": {
        "id": "V6YEh744viqd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "559010ca-5e30-4894-9663-6f4c0a11b80e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset:  hw1\n",
            "Correctly predicted: 390, total test examples: 463, total predicted examples: 463\n",
            "Correctly predicted: 369, total test examples: 478, total predicted examples: 478\n",
            "Dataset:  enron1\n",
            "Correctly predicted: 374, total test examples: 450, total predicted examples: 450\n",
            "Correctly predicted: 333, total test examples: 456, total predicted examples: 456\n",
            "Dataset:  enron4\n",
            "Correctly predicted: 493, total test examples: 535, total predicted examples: 535\n",
            "Correctly predicted: 496, total test examples: 543, total predicted examples: 543\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for k in data:\n",
        "    # evalution on training data\n",
        "    # print(k)\n",
        "    Y = data[k]['train']['ham']['Y'] + data[k]['train']['spam']['Y']\n",
        "    # print(np.shape(Y))\n",
        "    # print(np.shape(bernoulli_Y_pred_dict[\"train\"][k]))\n",
        "    metrics_dict[\"train\"][k] = get_evaluation_metrics(Y, bernoulli_Y_pred_dict[\"train\"][k])\n",
        "\n",
        "    # evaluation on testing data\n",
        "    Y = data[k]['test']['ham']['Y'] + data[k]['test']['spam']['Y']\n",
        "    # print(np.shape(Y))\n",
        "    # print(np.shape(bernoulli_Y_pred_dict[\"test\"][k]))\n",
        "    metrics_dict[\"test\"][k] = get_evaluation_metrics(Y, bernoulli_Y_pred_dict[\"test\"][k])\n",
        "print(print_metrics_dict(metrics_dict))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbK-Jf7-bMkr",
        "outputId": "a376400b-907e-4de2-8927-f9d4064ac613"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Set: train, Dataset: hw1 -> Accuracy: 0.8423326133909287, Precision: 0.9464285714285714, Recall: 0.43089430894308944, F1-Score: 0.5921787709497207\n",
            "Set: train, Dataset: enron1 -> Accuracy: 0.8311111111111111, Precision: 1.0, Recall: 0.4198473282442748, F1-Score: 0.5913978494623656\n",
            "Set: train, Dataset: enron4 -> Accuracy: 0.9214953271028037, Precision: 0.9347826086956522, Recall: 0.9626865671641791, F1-Score: 0.948529411764706\n",
            "Set: test, Dataset: hw1 -> Accuracy: 0.7719665271966527, Precision: 0.8888888888888888, Recall: 0.18461538461538463, F1-Score: 0.3057324840764331\n",
            "Set: test, Dataset: enron1 -> Accuracy: 0.7302631578947368, Precision: 0.90625, Recall: 0.19463087248322147, F1-Score: 0.3204419889502762\n",
            "Set: test, Dataset: enron4 -> Accuracy: 0.9134438305709024, Precision: 0.8926940639269406, Recall: 1.0, F1-Score: 0.9433051869722556\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Logistic Regression</h1>"
      ],
      "metadata": {
        "id": "66VoPBsyvi-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def bow_bernoulli(local_all_text, local_vocab):\n",
        "    bag_of_words_corpus = []\n",
        "    bernoulli_corpus = []\n",
        "\n",
        "    for text in local_all_text:        \n",
        "        _, words_list = clean_text_document(text)   \n",
        "        bag_of_words = np.zeros(len(local_vocab))  \n",
        "        bernoulli = np.zeros(len(local_vocab))        \n",
        "        for word in words_list:            \n",
        "            for i, w in enumerate(local_vocab):                \n",
        "                if word == w:                     \n",
        "                    bag_of_words[i] += 1  \n",
        "                    bernoulli[i] = 1             \n",
        "                    # print(\"{0}\\n{1}\\n\".format(_, np.array(bag_of_words)))\n",
        "        bag_of_words_corpus.append(bag_of_words)\n",
        "        bernoulli_corpus.append(bernoulli)\n",
        "        # print(\"{0}\\n{1}\\n\".format(cleaned_text, np.array(bag_of_words)))\n",
        "    # print(np.shape(bag_of_words_corpus))\n",
        "    # print(np.shape(bernoulli_corpus))\n",
        "    return bag_of_words_corpus, bernoulli_corpus\n"
      ],
      "metadata": {
        "id": "nD8aAKShICya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class log_regression():\n",
        "    def __init__(self, lr = 0.1, max_iter = 64, reg_para = 0.01):\n",
        "        self.lr = lr\n",
        "        self.max_iter = max_iter\n",
        "        self.reg_para = reg_para\n",
        "\n",
        "\n",
        "    def fit(self, X, Y):\n",
        "        # First lets define a weight matrix which includes w0 for bais term\n",
        "        self.weights = np.zeros(X.shape[1] + 1)\n",
        "\n",
        "        # Add x0 ie. bais term of 1 in each X\n",
        "        X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
        "\n",
        "        for i in range(self.max_iter):\n",
        "            product = np.matmul(X, self.weights)\n",
        "\n",
        "            h = 1 / (1 + np.exp(-product))\n",
        "            # print(h)\n",
        "            no_of_features = X.shape[1]\n",
        "\n",
        "            # Update Weights\n",
        "            grad = self.lr * np.matmul(X.transpose(), (Y - h))\n",
        "            gradient_regularization = self.lr * self.reg_para * self.weights\n",
        "            # print(grad.shape)\n",
        "            # print(gradient_regularization.shape)\n",
        "            self.weights = self.weights + grad - gradient_regularization\n",
        "            # self.weights += self.lr * np.sum(X.transpose() * (Y - h)) - self.lr * self.reg_para * self.weights\n",
        "\n",
        "        return self\n",
        "\n",
        "    \n",
        "    def predict(self, X):\n",
        "        prob = np.matmul(X, self.weights[1:]) + self.weights[0]\n",
        "\n",
        "        prob = 1 / (1 + np.exp(-prob))  \n",
        "\n",
        "        return prob\n"
      ],
      "metadata": {
        "id": "Yv7CMzpsvmak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>VALIDATING FOR VALUES OF LAMBDA</H1>"
      ],
      "metadata": {
        "id": "j3m2c24oiGQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "for k in lr_data:\n",
        "    lr_data[k][\"validation\"] = dict()\n",
        "    lr_data[k][\"train0.7\"] = dict()\n",
        "\n",
        "# vocab_dict = {\"hw1\": None, \"enron1\": None, \"enron4\": None}\n",
        "lr_dict = {\"ham\": {\"hw1\": None, \"enron1\": None, \"enron4\": None}, \"spam\": {\"hw1\": None, \"enron1\": None, \"enron4\": None}}\n",
        "# prior_dict = {\"ham\": {\"hw1\": None, \"enron1\": None, \"enron4\": None}, \"spam\": {\"hw1\": None, \"enron1\": None, \"enron4\": None}}\n",
        "# lr_cond_prob_dict = {\"ham\": {\"hw1\": None, \"enron1\": None, \"enron4\": None}, \"spam\": {\"hw1\": None, \"enron1\": None, \"enron4\": None}}\n",
        "lr_Y_pred_dict = {\"train\": {\"hw1\": [], \"enron1\": [], \"enron4\": []}, \"train0.7\": {\"hw1\": [], \"enron1\": [], \"enron4\": []}, \"validation\": {\"hw1\": [], \"enron1\": [], \"enron4\": []}, \"test\": {\"hw1\": [], \"enron1\": [], \"enron4\": []}}\n",
        "# metrics_dict = {\"train\": {\"hw1\": [], \"enron1\": [], \"enron4\": []}, \"validation\": {\"hw1\": [], \"enron1\": [], \"enron4\": []}, \"test\": {\"hw1\": [], \"enron1\": [], \"enron4\": []}}\n",
        "                  \n",
        "lr_metrics_dict = {\"train\": {\"hw1\": [], \"enron1\": [], \"enron4\": []}, \"validation\": {\"hw1\": [], \"enron1\": [], \"enron4\": []}, \"test\": {\"hw1\": [], \"enron1\": [], \"enron4\": []}}\n",
        "def lr_for_lambda(data, lambda_list):\n",
        "    seed = 69\n",
        "    np.random.seed(seed)\n",
        "    \n",
        "    for k in data:\n",
        "    # k = \"hw1\"\n",
        "    # if k == \"hw1\":\n",
        "        # Assign X and Y and spam and ham texts\n",
        "        ham_all_text = data[k]['train']['ham']['X']\n",
        "        spam_all_text = data[k]['train']['spam']['X']\n",
        "        # X = ham_all_text + spam_all_text\n",
        "        Y_all = data[k]['train']['ham']['Y'] + data[k]['train']['spam']['Y']\n",
        "\n",
        "        # Call vocab builder and save it in vocab_dict\n",
        "        # vocab_dict[k] = vocab_builder(X)\n",
        "        # print(len(vocab_dict[k]))\n",
        "\n",
        "        # Make bow and save respective ham and spam in the dict\n",
        "        lr_dict[\"ham\"][k] = bow_bernoulli(ham_all_text, vocab_dict[k])\n",
        "        lr_dict[\"spam\"][k] = bow_bernoulli(spam_all_text, vocab_dict[k])\n",
        "\n",
        "\n",
        "        X_all0 = np.array(lr_dict[\"ham\"][k][0] + lr_dict[\"spam\"][k][0])\n",
        "        X_all1 = np.array(lr_dict[\"ham\"][k][1] + lr_dict[\"spam\"][k][1])\n",
        "\n",
        "        # Randomize the X and Y before training\n",
        "        np.random.shuffle(X_all0)\n",
        "        np.random.shuffle(X_all1)\n",
        "        np.random.shuffle(Y_all)\n",
        "\n",
        "        # Store complete training set in \"train\"\n",
        "        lr_data[k][\"train\"][\"X\"] = [X_all0, X_all1]\n",
        "        lr_data[k][\"train\"][\"Y\"] = Y_all\n",
        "\n",
        "\n",
        "        # Split train into train0.7+validation\n",
        "        lr_data[k][\"train0.7\"][\"X\"] = [X_all0[:math.floor(0.7 * len(X_all0))], X_all1[:math.floor(0.7 * len(X_all1))]]\n",
        "        lr_data[k][\"validation\"][\"X\"] = [X_all0[math.floor(0.7 * len(X_all0)):], X_all1[math.floor(0.7 * len(X_all1)):]]\n",
        "\n",
        "        lr_data[k][\"train0.7\"][\"Y\"] = Y_all[:math.floor(0.7 * len(Y_all))]\n",
        "        lr_data[k][\"validation\"][\"Y\"] = Y_all[math.floor(0.7 * len(Y_all)):]\n",
        "\n",
        "        \n",
        "        # Evalute on test data\n",
        "        X_test = data[k]['test']['ham']['X'] + data[k]['test']['spam']['X']\n",
        "        Y_test = data[k]['test']['ham']['Y'] + data[k]['test']['spam']['Y']\n",
        "        X_test = np.array(bow_bernoulli(X_test, vocab_dict[k]))\n",
        "\n",
        "        # Save test in lr_data. X is after applying bow_bernoulli\n",
        "        lr_data[k][\"test\"][\"X\"] = X_test\n",
        "        lr_data[k][\"test\"][\"Y\"] = Y_test\n",
        "\n",
        "###################################################################\n",
        "\n",
        "    for lambd in lambda_list:\n",
        "        lr_Y_pred_dict = {\"train\": {\"hw1\": [], \"enron1\": [], \"enron4\": []}, \"train0.7\": {\"hw1\": [], \"enron1\": [], \"enron4\": []}, \"validation\": {\"hw1\": [], \"enron1\": [], \"enron4\": []}, \"test\": {\"hw1\": [], \"enron1\": [], \"enron4\": []}}\n",
        "        # print(\"LAMBDA: \", lambd)\n",
        "        for k in data:\n",
        "            # Loop for bernoulli and BOW:\n",
        "            for b in range(2):\n",
        "                # print(\"Ber or Bow: \", b)\n",
        "                # print(\"K: \", k)\n",
        "                # # Set X and Y as training parts\n",
        "                X = lr_data[k][\"train0.7\"][\"X\"][b]\n",
        "                Y = lr_data[k][\"train0.7\"][\"Y\"]\n",
        "\n",
        "\n",
        "\n",
        "                # print(X.shape)\n",
        "                # print(X)\n",
        "                output = log_regression(reg_para=lambd).fit(X, Y)\n",
        "\n",
        "                \n",
        "                # TRAINING ON TRAIN 0.7 PART OF DATA\n",
        "                correct = 0\n",
        "                lr_Y_pred_dict[\"train0.7\"][k].append([])\n",
        "                # lr_Y_pred_dict[\"train\"][k] = Y.copy()\n",
        "                for i, d in enumerate(X):\n",
        "                    # print(len(d))\n",
        "                    p = output.predict(d)\n",
        "                    # print(p)\n",
        "                    if p > 0.5:\n",
        "                        lr_Y_pred_dict[\"train0.7\"][k][b].append(1)\n",
        "                        if Y[i] == 1:\n",
        "                            correct += 1\n",
        "                    else:\n",
        "                        lr_Y_pred_dict[\"train0.7\"][k][b].append(0)\n",
        "                        if Y[i] == 0:\n",
        "                            correct += 1\n",
        "                    \n",
        "                # print(correct, len(Y), len(lr_Y_pred_dict[\"train\"][k]))\n",
        "\n",
        "                # EVALUTE ON VALIDATION SET\n",
        "                X = lr_data[k][\"validation\"][\"X\"][b]\n",
        "                Y = lr_data[k][\"validation\"][\"Y\"]\n",
        "                correct = 0\n",
        "                lr_Y_pred_dict[\"validation\"][k].append([])\n",
        "\n",
        "                # lr_Y_pred_dict[\"train\"][k] = Y.copy()\n",
        "                for i, d in enumerate(X):\n",
        "                    # print(len(d))\n",
        "                    p = output.predict(d)\n",
        "                    # print(p)\n",
        "                    if p > 0.5:\n",
        "                        lr_Y_pred_dict[\"validation\"][k][b].append(1)\n",
        "                        if Y[i] == 1:\n",
        "                            correct += 1\n",
        "                    else:\n",
        "                        lr_Y_pred_dict[\"validation\"][k][b].append(0)\n",
        "                        if Y[i] == 0:\n",
        "                            correct += 1\n",
        "                    \n",
        "                # print(correct, len(Y), len(lr_Y_pred_dict[\"validation\"][k]))\n",
        "                if b == 0:\n",
        "                    ber_bag = \"Bag Of Words model\"\n",
        "                else:\n",
        "                    ber_bag = \"Bernoulli model\"\n",
        "\n",
        "                print(f'Lambda: {lambd}, Model: {ber_bag}, Dataset: {k} -> ', end=\" \")\n",
        "                print('Accuracy: {:.2f}'.format(accuracy_score(Y, lr_Y_pred_dict[\"validation\"][k][b])))\n",
        "\n",
        "        \n",
        "lambda_list = [0.0001, 0.001, 0.01, 0.1]\n",
        "lr_for_lambda(data, lambda_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mSh47xgnVvL",
        "outputId": "49272f2b-3447-4cff-cded-fd7a29f9bd9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lambda: 0.001, Model: Bag Of Words model, Dataset: hw1 ->  Accuracy: 0.59\n",
            "Lambda: 0.001, Model: Bag Of Words model, Dataset: hw1 ->  Accuracy: 0.71\n",
            "Lambda: 0.001, Model: Bag Of Words model, Dataset: enron1 ->  Accuracy: 0.60\n",
            "Lambda: 0.001, Model: Bag Of Words model, Dataset: enron1 ->  Accuracy: 0.56\n",
            "Lambda: 0.001, Model: Bag Of Words model, Dataset: enron4 ->  Accuracy: 0.63\n",
            "Lambda: 0.001, Model: Bag Of Words model, Dataset: enron4 ->  Accuracy: 0.75\n",
            "Lambda: 0.01, Model: Bag Of Words model, Dataset: hw1 ->  Accuracy: 0.63\n",
            "Lambda: 0.01, Model: Bag Of Words model, Dataset: hw1 ->  Accuracy: 0.71\n",
            "Lambda: 0.01, Model: Bag Of Words model, Dataset: enron1 ->  Accuracy: 0.61\n",
            "Lambda: 0.01, Model: Bag Of Words model, Dataset: enron1 ->  Accuracy: 0.56\n",
            "Lambda: 0.01, Model: Bag Of Words model, Dataset: enron4 ->  Accuracy: 0.68\n",
            "Lambda: 0.01, Model: Bag Of Words model, Dataset: enron4 ->  Accuracy: 0.73\n",
            "Lambda: 0.1, Model: Bag Of Words model, Dataset: hw1 ->  Accuracy: 0.66\n",
            "Lambda: 0.1, Model: Bag Of Words model, Dataset: hw1 ->  Accuracy: 0.72\n",
            "Lambda: 0.1, Model: Bag Of Words model, Dataset: enron1 ->  Accuracy: 0.58\n",
            "Lambda: 0.1, Model: Bag Of Words model, Dataset: enron1 ->  Accuracy: 0.50\n",
            "Lambda: 0.1, Model: Bag Of Words model, Dataset: enron4 ->  Accuracy: 0.65\n",
            "Lambda: 0.1, Model: Bag Of Words model, Dataset: enron4 ->  Accuracy: 0.58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<H1>TRAINING ON THE BEST LAMBDA VALUE AND TESTING ON THE TEST SET</H1>"
      ],
      "metadata": {
        "id": "ZLdBzF7riLGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for k in lr_data:\n",
        "#     lr_data[k][\"validation\"] = dict()\n",
        "#     lr_data[k][\"train0.7\"] = dict()\n",
        "\n",
        "# vocab_dict = {\"hw1\": None, \"enron1\": None, \"enron4\": None}\n",
        "# lr_dict = {\"ham\": {\"hw1\": None, \"enron1\": None, \"enron4\": None}, \"spam\": {\"hw1\": None, \"enron1\": None, \"enron4\": None}}\n",
        "# prior_dict = {\"ham\": {\"hw1\": None, \"enron1\": None, \"enron4\": None}, \"spam\": {\"hw1\": None, \"enron1\": None, \"enron4\": None}}\n",
        "# lr_cond_prob_dict = {\"ham\": {\"hw1\": None, \"enron1\": None, \"enron4\": None}, \"spam\": {\"hw1\": None, \"enron1\": None, \"enron4\": None}}\n",
        "lr_Y_pred_dict = {\"train\": {\"hw1\": [], \"enron1\": [], \"enron4\": []}, \"test\": {\"hw1\": [], \"enron1\": [], \"enron4\": []}}\n",
        "# metrics_dict = {\"train\": {\"hw1\": [], \"enron1\": [], \"enron4\": []}, \"validation\": {\"hw1\": [], \"enron1\": [], \"enron4\": []}, \"test\": {\"hw1\": [], \"enron1\": [], \"enron4\": []}}\n",
        "                  \n",
        "lr_metrics_dict = {\"train\": {\"hw1\": [], \"enron1\": [], \"enron4\": []}, \"test\": {\"hw1\": [], \"enron1\": [], \"enron4\": []}}\n",
        "def lr(data, lambd, l_rate):\n",
        "    # seed = np.random.randint(0, 10000)\n",
        "    # np.random.seed(seed)\n",
        "    for k in data:\n",
        "        # Bow or Bernoulli:\n",
        "        for b in range(2):\n",
        "        \n",
        "            # Set X and Y as training parts\n",
        "            X = lr_data[k][\"train\"][\"X\"][b]\n",
        "            Y = lr_data[k][\"train\"][\"Y\"]\n",
        "\n",
        "\n",
        "\n",
        "            # print(X.shape)\n",
        "            # print(X)\n",
        "            output = log_regression(reg_para=lambd, lr=l_rate).fit(X, Y)\n",
        "\n",
        "            \n",
        "            # TRAINING ON TRAIN 0.7 PART OF DATA\n",
        "            correct = 0\n",
        "            lr_Y_pred_dict[\"train\"][k].append([])\n",
        "            # lr_Y_pred_dict[\"train\"][k] = Y.copy()\n",
        "            for i, d in enumerate(X):\n",
        "                # print(len(d))\n",
        "                p = output.predict(d)\n",
        "                # print(p)\n",
        "                if p > 0.5:\n",
        "                    lr_Y_pred_dict[\"train\"][k][b].append(1)\n",
        "                    if Y[i] == 1:\n",
        "                        correct += 1\n",
        "                else:\n",
        "                    lr_Y_pred_dict[\"train\"][k][b].append(0)\n",
        "                    if Y[i] == 0:\n",
        "                        correct += 1\n",
        "                \n",
        "            # print(\"BOW or BION and K: TRAIN:\", b, k)\n",
        "            # print(correct, len(Y), len(lr_Y_pred_dict[\"train\"][k][b]))\n",
        "            if b == 0:\n",
        "                ber_bag = \"Bag Of Words model\"\n",
        "            else:\n",
        "                ber_bag = \"Bernoulli model\"\n",
        "\n",
        "            print(f'TRAIN: Lambda: {lambd}, Model: {ber_bag}, Dataset: {k} -> ', end=\" \")\n",
        "            print('Accuracy: {:.2f}'.format(accuracy_score(Y, lr_Y_pred_dict[\"train\"][k][b])))\n",
        "\n",
        "            # Save test in lr_data. X is after applying bow_bernoulli\n",
        "            X_test = lr_data[k][\"test\"][\"X\"] \n",
        "            Y_test = lr_data[k][\"test\"][\"Y\"] \n",
        "\n",
        "\n",
        "            test_correct = 0\n",
        "            lr_Y_pred_dict[\"test\"][k].append([])\n",
        "            for i, d in enumerate(X_test[0]):\n",
        "                # print(len(d))\n",
        "                # print(i, d)\n",
        "                p = output.predict(d)\n",
        "                # print(p)\n",
        "                if p > 0.5:\n",
        "                    lr_Y_pred_dict[\"test\"][k][b].append(1)\n",
        "                    if Y_test[i] == 1:\n",
        "                        test_correct += 1\n",
        "                else:\n",
        "                    lr_Y_pred_dict[\"test\"][k][b].append(0)\n",
        "                    if Y_test[i] == 0:\n",
        "                        test_correct += 1\n",
        "                \n",
        "            # print(\"BOW or BION and K: TEST:\", b, k)\n",
        "            # print(test_correct, len(Y_test), len(lr_Y_pred_dict[\"test\"][k][b]))\n",
        "            if b == 0:\n",
        "                ber_bag = \"Bag Of Words model\"\n",
        "            else:\n",
        "                ber_bag = \"Bernoulli model\"\n",
        "\n",
        "            print(f'TEST: Lambda: {lambd}, Model: {ber_bag}, Dataset: {k} -> ', end=\" \")\n",
        "            print('Accuracy: {:.2f}'.format(accuracy_score(Y_test, lr_Y_pred_dict[\"test\"][k][b])))\n",
        "\n",
        "            # metrics_dict[\"train\"][k] = get_evaluation_metrics(Y, Y_pred_dict[\"train\"][k])\n",
        "        \n",
        "\n",
        "        \n",
        "\n",
        "lr(data, 0.1, 0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Dv3JSecrNjH",
        "outputId": "c14c246c-2701-4ff5-d98a-fcfb87e24784"
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: Lambda: 0.1, Model: Bag Of Words model, Dataset: hw1 ->  Accuracy: 0.98\n",
            "TEST: Lambda: 0.1, Model: Bag Of Words model, Dataset: hw1 ->  Accuracy: 0.64\n",
            "TRAIN: Lambda: 0.1, Model: Bernoulli model, Dataset: hw1 ->  Accuracy: 0.97\n",
            "TEST: Lambda: 0.1, Model: Bernoulli model, Dataset: hw1 ->  Accuracy: 0.69\n",
            "TRAIN: Lambda: 0.1, Model: Bag Of Words model, Dataset: enron1 ->  Accuracy: 0.98\n",
            "TEST: Lambda: 0.1, Model: Bag Of Words model, Dataset: enron1 ->  Accuracy: 0.53\n",
            "TRAIN: Lambda: 0.1, Model: Bernoulli model, Dataset: enron1 ->  Accuracy: 0.97\n",
            "TEST: Lambda: 0.1, Model: Bernoulli model, Dataset: enron1 ->  Accuracy: 0.51\n",
            "TRAIN: Lambda: 0.1, Model: Bag Of Words model, Dataset: enron4 ->  Accuracy: 0.93\n",
            "TEST: Lambda: 0.1, Model: Bag Of Words model, Dataset: enron4 ->  Accuracy: 0.77\n",
            "TRAIN: Lambda: 0.1, Model: Bernoulli model, Dataset: enron4 ->  Accuracy: 0.97\n",
            "TEST: Lambda: 0.1, Model: Bernoulli model, Dataset: enron4 ->  Accuracy: 0.68\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for b in range(2):\n",
        "    if b == 0:\n",
        "        ber_bag = \"Bag Of Words model\"\n",
        "    else:\n",
        "        ber_bag = \"Bernoulli model\"\n",
        "    print(f'Model: {ber_bag} -> ')\n",
        "    metrics_dict = {\"train\": {\"hw1\": [], \"enron1\": [], \"enron4\": []}, \"test\": {\"hw1\": [], \"enron1\": [], \"enron4\": []}}\n",
        "\n",
        "    for k in lr_data:\n",
        "        # evalution on training data\n",
        "        # print(k)\n",
        "        Y = lr_data[k][\"train\"][\"Y\"]\n",
        "        # print(np.shape(Y))\n",
        "        # print(np.shape(lr_Y_pred_dict[\"train\"][k]))\n",
        "        metrics_dict[\"train\"][k] = get_evaluation_metrics(Y, lr_Y_pred_dict[\"train\"][k][b])\n",
        "\n",
        "        # evaluation on testing data\n",
        "        Y = data[k]['test']['ham']['Y'] + data[k]['test']['spam']['Y']\n",
        "        # print(np.shape(Y))\n",
        "        # print(np.shape(lr_Y_pred_dict[\"test\"][k]))\n",
        "        metrics_dict[\"test\"][k] = get_evaluation_metrics(Y, lr_Y_pred_dict[\"test\"][k][b])\n",
        "    print(print_metrics_dict(metrics_dict))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTBAU5D6rNJ1",
        "outputId": "1bda7887-694e-421b-9fa2-9ac048ae0028"
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: Bag Of Words model -> \n",
            "Set: train, Dataset: hw1 -> Accuracy: 0.9762419006479481, Precision: 0.9590163934426229, Recall: 0.9512195121951219, F1-Score: 0.9551020408163264\n",
            "Set: train, Dataset: enron1 -> Accuracy: 0.9755555555555555, Precision: 0.9918032786885246, Recall: 0.9236641221374046, F1-Score: 0.9565217391304348\n",
            "Set: train, Dataset: enron4 -> Accuracy: 0.9345794392523364, Precision: 0.9717223650385605, Recall: 0.9402985074626866, F1-Score: 0.9557522123893806\n",
            "Set: test, Dataset: hw1 -> Accuracy: 0.6443514644351465, Precision: 0.2619047619047619, Recall: 0.16923076923076924, F1-Score: 0.205607476635514\n",
            "Set: test, Dataset: enron1 -> Accuracy: 0.5307017543859649, Precision: 0.23140495867768596, Recall: 0.18791946308724833, F1-Score: 0.2074074074074074\n",
            "Set: test, Dataset: enron4 -> Accuracy: 0.7716390423572744, Precision: 0.801354401805869, Recall: 0.907928388746803, F1-Score: 0.8513189448441246\n",
            "\n",
            "Model: Bernoulli model -> \n",
            "Set: train, Dataset: hw1 -> Accuracy: 0.9654427645788337, Precision: 0.9908256880733946, Recall: 0.8780487804878049, F1-Score: 0.9310344827586207\n",
            "Set: train, Dataset: enron1 -> Accuracy: 0.9666666666666667, Precision: 0.9754098360655737, Recall: 0.9083969465648855, F1-Score: 0.9407114624505928\n",
            "Set: train, Dataset: enron4 -> Accuracy: 0.9663551401869159, Precision: 0.9571428571428572, Recall: 1.0, F1-Score: 0.9781021897810218\n",
            "Set: test, Dataset: hw1 -> Accuracy: 0.6903765690376569, Precision: 0.29545454545454547, Recall: 0.1, F1-Score: 0.14942528735632182\n",
            "Set: test, Dataset: enron1 -> Accuracy: 0.5087719298245614, Precision: 0.10526315789473684, Recall: 0.06711409395973154, F1-Score: 0.08196721311475409\n",
            "Set: test, Dataset: enron4 -> Accuracy: 0.6813996316758748, Precision: 0.7339055793991416, Recall: 0.8746803069053708, F1-Score: 0.7981330221703618\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the randomizatoin of array"
      ],
      "metadata": {
        "id": "tHAHOWPQhNwE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eOYwEVX6_-SQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>SGD-CLASSIFIER</h1>"
      ],
      "metadata": {
        "id": "v4mK1q-0Trjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "isI63tJCTv2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mICzsuSsT5bQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using GridSearch to tune hyperparameters"
      ],
      "metadata": {
        "id": "wzmU_IREViPl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = {\"penalty\": [\"none\", \"l1\", \"l2\"],\n",
        "              \"alpha\": [0.0001, 0.001, 0.01, 0.1],\n",
        "              \"loss\": [\"log\", \"squared_hinge\", \"log\"]\n",
        "              }\n",
        "\n",
        "sgd = SGDClassifier(max_iter=64)\n",
        "grid_search = GridSearchCV(sgd, param_grid=parameters)\n",
        "\n",
        "for k in data:\n",
        "    # For BOW and Bernoulli\n",
        "    for b in range(2):\n",
        "        grid_search.fit(lr_data[k][\"train\"][\"X\"][b], lr_data[k][\"train\"][\"Y\"])\n",
        "        if b == 0:\n",
        "            ber_bag = \"Bag Of Words model\"\n",
        "        else:\n",
        "            ber_bag = \"Bernoulli model\"\n",
        "        print(f'Dataset: {k}, and Model: {ber_bag} -> Best Parameters: ', end=\" \")\n",
        "        print(grid_search.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ni8Omo3eVhuR",
        "outputId": "2a7f417b-3e99-4e03-b9d7-d10b2d748bd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: hw1, and Model: Bag Of Words model -> Best Parameters:  {'alpha': 0.1, 'loss': 'log', 'penalty': 'l1'}\n",
            "Dataset: hw1, and Model: Bernoulli model -> Best Parameters:  {'alpha': 0.01, 'loss': 'log', 'penalty': 'l2'}\n",
            "Dataset: enron1, and Model: Bag Of Words model -> Best Parameters:  {'alpha': 0.1, 'loss': 'log', 'penalty': 'l1'}\n",
            "Dataset: enron1, and Model: Bernoulli model -> Best Parameters:  {'alpha': 0.1, 'loss': 'log', 'penalty': 'l1'}\n",
            "Dataset: enron4, and Model: Bag Of Words model -> Best Parameters:  {'alpha': 0.01, 'loss': 'log', 'penalty': 'l2'}\n",
            "Dataset: enron4, and Model: Bernoulli model -> Best Parameters:  {'alpha': 0.01, 'loss': 'log', 'penalty': 'l1'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using parameters from GRIDSEARCH on SGDClassifier"
      ],
      "metadata": {
        "id": "ozRynLdrfVVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sgd = SGDClassifier(loss=\"log\", penalty=\"l2\", alpha=0.01, max_iter=64)\n",
        "\n",
        "# For BOW and Bernoulli\n",
        "for b in range(2):\n",
        "    if b == 0:\n",
        "        ber_bag = \"Bag Of Words model\"\n",
        "    else:\n",
        "        ber_bag = \"Bernoulli model\"\n",
        "    print(f'Model: {ber_bag} ->')\n",
        "    metrics_dict = {\"train\": {\"hw1\": [], \"enron1\": [], \"enron4\": []}, \"test\": {\"hw1\": [], \"enron1\": [], \"enron4\": []}}\n",
        "\n",
        "    for k in data:\n",
        "        sgd.fit(lr_data[k][\"train\"][\"X\"][b], lr_data[k][\"train\"][\"Y\"])\n",
        "\n",
        "        Y_pred = sgd.predict(lr_data[k][\"train\"][\"X\"][b])\n",
        "        metrics_dict[\"train\"][k] = get_evaluation_metrics(lr_data[k][\"train\"][\"Y\"], Y_pred)\n",
        "\n",
        "        Y_pred = sgd.predict(lr_data[k][\"test\"][\"X\"][b])\n",
        "        if b == 0:\n",
        "            ber_bag = \"Bag Of Words model\"\n",
        "        else:\n",
        "            ber_bag = \"Bernoulli model\"\n",
        "        print(f'Dataset: {k}, and Model: {ber_bag} -> ', end=\" \")\n",
        "        print('Accuracy: {:.2f}'.format(accuracy_score(lr_data[k][\"test\"][\"Y\"], Y_pred)))\n",
        "        metrics_dict[\"test\"][k] = get_evaluation_metrics(lr_data[k][\"test\"][\"Y\"], Y_pred)\n",
        "    print(print_metrics_dict(metrics_dict))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0C4CxXW7ai6C",
        "outputId": "f07a3fad-6b59-4494-ff9b-a33eefc4dbb8"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: Bag Of Words model ->\n",
            "Dataset: hw1, and Model: Bag Of Words model ->  Accuracy: 0.68\n",
            "Dataset: enron1, and Model: Bag Of Words model ->  Accuracy: 0.57\n",
            "Dataset: enron4, and Model: Bag Of Words model ->  Accuracy: 0.71\n",
            "Set: train, Dataset: hw1 -> Accuracy: 0.9481641468682506, Precision: 1.0, Recall: 0.8048780487804879, F1-Score: 0.8918918918918919\n",
            "Set: train, Dataset: enron1 -> Accuracy: 0.9622222222222222, Precision: 0.9913793103448276, Recall: 0.8778625954198473, F1-Score: 0.9311740890688259\n",
            "Set: train, Dataset: enron4 -> Accuracy: 0.9439252336448598, Precision: 0.9305555555555556, Recall: 1.0, F1-Score: 0.9640287769784173\n",
            "Set: test, Dataset: hw1 -> Accuracy: 0.6820083682008368, Precision: 0.25, Recall: 0.08461538461538462, F1-Score: 0.1264367816091954\n",
            "Set: test, Dataset: enron1 -> Accuracy: 0.5657894736842105, Precision: 0.12307692307692308, Recall: 0.053691275167785234, F1-Score: 0.07476635514018691\n",
            "Set: test, Dataset: enron4 -> Accuracy: 0.7108655616942909, Precision: 0.7276264591439688, Recall: 0.9565217391304348, F1-Score: 0.8265193370165747\n",
            "\n",
            "Model: Bernoulli model ->\n",
            "Dataset: hw1, and Model: Bernoulli model ->  Accuracy: 0.71\n",
            "Dataset: enron1, and Model: Bernoulli model ->  Accuracy: 0.63\n",
            "Dataset: enron4, and Model: Bernoulli model ->  Accuracy: 0.69\n",
            "Set: train, Dataset: hw1 -> Accuracy: 0.9330453563714903, Precision: 1.0, Recall: 0.7479674796747967, F1-Score: 0.855813953488372\n",
            "Set: train, Dataset: enron1 -> Accuracy: 0.9355555555555556, Precision: 0.9903846153846154, Recall: 0.7862595419847328, F1-Score: 0.8765957446808511\n",
            "Set: train, Dataset: enron4 -> Accuracy: 0.9271028037383178, Precision: 0.9153318077803204, Recall: 0.9950248756218906, F1-Score: 0.9535160905840286\n",
            "Set: test, Dataset: hw1 -> Accuracy: 0.7112970711297071, Precision: 0.0, Recall: 0.0, F1-Score: nan\n",
            "Set: test, Dataset: enron1 -> Accuracy: 0.6293859649122807, Precision: 0.16666666666666666, Recall: 0.03355704697986577, F1-Score: 0.055865921787709494\n",
            "Set: test, Dataset: enron4 -> Accuracy: 0.6887661141804788, Precision: 0.7126436781609196, Recall: 0.9514066496163683, F1-Score: 0.8148959474260679\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tXy7AvbbfbVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sgd = SGDClassifier(loss=\"log\", penalty=\"l1\", alpha=0.01, max_iter=64)\n",
        "\n",
        "# For BOW and Bernoulli\n",
        "for b in range(2):\n",
        "    if b == 0:\n",
        "        ber_bag = \"Bag Of Words model\"\n",
        "    else:\n",
        "        ber_bag = \"Bernoulli model\"\n",
        "    print(f'Model: {ber_bag} ->')\n",
        "    metrics_dict = {\"train\": {\"hw1\": [], \"enron1\": [], \"enron4\": []}, \"test\": {\"hw1\": [], \"enron1\": [], \"enron4\": []}}\n",
        "\n",
        "    for k in data:\n",
        "        sgd.fit(lr_data[k][\"train\"][\"X\"][b], lr_data[k][\"train\"][\"Y\"])\n",
        "\n",
        "        Y_pred = sgd.predict(lr_data[k][\"train\"][\"X\"][b])\n",
        "        metrics_dict[\"train\"][k] = get_evaluation_metrics(lr_data[k][\"train\"][\"Y\"], Y_pred)\n",
        "\n",
        "        Y_pred = sgd.predict(lr_data[k][\"test\"][\"X\"][b])\n",
        "        if b == 0:\n",
        "            ber_bag = \"Bag Of Words model\"\n",
        "        else:\n",
        "            ber_bag = \"Bernoulli model\"\n",
        "        print(f'Dataset: {k}, and Model: {ber_bag} -> ', end=\" \")\n",
        "        print('Accuracy: {:.2f}'.format(accuracy_score(lr_data[k][\"test\"][\"Y\"], Y_pred)))\n",
        "        metrics_dict[\"test\"][k] = get_evaluation_metrics(lr_data[k][\"test\"][\"Y\"], Y_pred)\n",
        "    print(print_metrics_dict(metrics_dict))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DK9yxhVY-fSZ",
        "outputId": "69b5557f-23bf-4585-d9b0-d57d1d1a46ef"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: Bag Of Words model ->\n",
            "Dataset: hw1, and Model: Bag Of Words model ->  Accuracy: 0.68\n",
            "Dataset: enron1, and Model: Bag Of Words model ->  Accuracy: 0.63\n",
            "Dataset: enron4, and Model: Bag Of Words model ->  Accuracy: 0.68\n",
            "Set: train, Dataset: hw1 -> Accuracy: 0.7624190064794817, Precision: 0.76, Recall: 0.15447154471544716, F1-Score: 0.25675675675675674\n",
            "Set: train, Dataset: enron1 -> Accuracy: 0.7377777777777778, Precision: 0.782608695652174, Recall: 0.13740458015267176, F1-Score: 0.2337662337662338\n",
            "Set: train, Dataset: enron4 -> Accuracy: 0.7775700934579439, Precision: 0.7779960707269156, Recall: 0.9850746268656716, F1-Score: 0.8693743139407244\n",
            "Set: test, Dataset: hw1 -> Accuracy: 0.6820083682008368, Precision: 0.041666666666666664, Recall: 0.007692307692307693, F1-Score: 0.012987012987012988\n",
            "Set: test, Dataset: enron1 -> Accuracy: 0.6271929824561403, Precision: 0.08, Recall: 0.013422818791946308, F1-Score: 0.022988505747126436\n",
            "Set: test, Dataset: enron4 -> Accuracy: 0.6777163904235728, Precision: 0.711764705882353, Recall: 0.928388746803069, F1-Score: 0.8057713651498335\n",
            "\n",
            "Model: Bernoulli model ->\n",
            "Dataset: hw1, and Model: Bernoulli model ->  Accuracy: 0.73\n",
            "Dataset: enron1, and Model: Bernoulli model ->  Accuracy: 0.67\n",
            "Dataset: enron4, and Model: Bernoulli model ->  Accuracy: 0.72\n",
            "Set: train, Dataset: hw1 -> Accuracy: 0.734341252699784, Precision: nan, Recall: 0.0, F1-Score: nan\n",
            "Set: train, Dataset: enron1 -> Accuracy: 0.7088888888888889, Precision: nan, Recall: 0.0, F1-Score: nan\n",
            "Set: train, Dataset: enron4 -> Accuracy: 0.7514018691588785, Precision: 0.7514018691588785, Recall: 1.0, F1-Score: 0.8580576307363927\n",
            "Set: test, Dataset: hw1 -> Accuracy: 0.7280334728033473, Precision: nan, Recall: 0.0, F1-Score: nan\n",
            "Set: test, Dataset: enron1 -> Accuracy: 0.6732456140350878, Precision: nan, Recall: 0.0, F1-Score: nan\n",
            "Set: test, Dataset: enron4 -> Accuracy: 0.7200736648250461, Precision: 0.7200736648250461, Recall: 1.0, F1-Score: 0.8372591006423983\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o4s6cKYKogL-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}